{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ea9f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path_dataset ='/home/marrakchi/TFM/TFM_env/Datasets/ExKaldiSERDataset/'\n",
    "path_dataset ='/home/marrakchi/TFM/TFM_env/Datasets/ExKaldiASRDatasetFormat/' \n",
    "ROOT_FOLDERS ='/home/marrakchi/TFM/TFM_env/Datasets/inter1sp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "610d8b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Kaldi root directory was not found in system PATH. You can appoint it:\n",
      "exkaldi.info.reset_kaldi_root( yourPath )\n",
      "If not, ERROR will occur when implementing some core functions.\n",
      "ExKaldiInfo(version='1.3.5.4', major='1', minor='3', patch='5', upload='4')\n",
      "Kaldi(version='5.5', major='5', minor='5')\n",
      "/kaldi\n"
     ]
    }
   ],
   "source": [
    "# Systems Libraries\n",
    "import os\n",
    "\n",
    "# For data processing\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from IPython.display import display, HTML # For displaying tables\n",
    "import string # for text processing\n",
    "import IPython as ipd\n",
    "\n",
    "# Import exkaldi package\n",
    "import exkaldi\n",
    "exkaldi.info.reset_kaldi_root('/kaldi')\n",
    "from exkaldi import declare\n",
    "\n",
    "# ExKaldi Configuration\n",
    "ExKaldiInfo = exkaldi.info\n",
    "print(ExKaldiInfo)\n",
    "print(ExKaldiInfo.KALDI)\n",
    "print(ExKaldiInfo.KALDI_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1241cd37",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba43532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spa1f001\n",
      "f\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/marrakchi/TFM/TFM_env/Datasets/ExKaldiSERDataset/wav.scp'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing data structure for ExKaldi (WAV file, text file)\n",
    "\n",
    "# Prepare ExKaldi files\n",
    "transcription = exkaldi.Transcription(name=\"trans\")\n",
    "utt2spk = exkaldi.ListTable(name=\"utt2spk\")\n",
    "spk2utt = exkaldi.ListTable(name=\"spk2utt\")\n",
    "wavScp = exkaldi.ListTable(name=\"wavScp\")\n",
    "\n",
    "# Path\n",
    "path_info = ROOT_FOLDERS+'index_sp/contents.lst'\n",
    "\n",
    "# Read dataset transcriptions\n",
    "X = pd.read_csv(path_info, sep=\"\\t\", header=0, usecols=[0,1,7], encoding='latin-1')\n",
    "\n",
    "# Remove all rows with Utterance value missing\n",
    "X.dropna(subset = ['LBO'], inplace=True)\n",
    "\n",
    "# PREPROCESS TRANSCRIPTIONS\n",
    "# Drop all containing digits:\n",
    "X = X.drop(X[X.LBO.str.contains(r'\\d+')].index)\n",
    "# Label to lower case\n",
    "X['LBO'] = X['LBO'].str.lower()\n",
    "# Remove punctuations:\n",
    "X['LBO'] = X['LBO'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "# Convert to plain text. Remove accents and any other symbols like ¨, æ, etc. \n",
    "X['LBO'] = X['LBO'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "# Rewrite directory column\n",
    "X['DIR'] = X.DIR.str.replace('\\\\', '/')\n",
    "# Mix columns 1 & 2 into one\n",
    "X['DIR'] = '/home/marrakchi/TFM/TFM_env/Datasets'+ X['DIR'].str.lower() + '/' + X['SRC'].str.lower()\n",
    "\n",
    "# Remove SRC column\n",
    "#X = X.drop(columns=['SRC'])\n",
    "\n",
    "#Convert to dataframe format and drop extra columns\n",
    "df = pd.DataFrame(X)\n",
    "df = df.drop(columns=['DIR'])\n",
    "df = df.reset_index(drop=True) # Refrech indexes\n",
    "\n",
    "for num,row in enumerate (df.SRC):\n",
    "    uttID = row.lower()\n",
    "    speaker = uttID[4]\n",
    "    if num == 0:\n",
    "        print(uttID)\n",
    "        print(speaker)\n",
    "    # 1. text\n",
    "    transcription[uttID] = df.LBO[num]\n",
    "    # 4. wav.scp\n",
    "    wavFilePath = os.path.join(new_path_dataset,'wav/',uttID +'.wav')\n",
    "    wavScp[uttID] = wavFilePath\n",
    "    # 2. utt2spk\n",
    "    utt2spk[uttID] = speaker\n",
    "    # 3. spk2utt\n",
    "    if speaker not in spk2utt.keys():\n",
    "        spk2utt[speaker] = f\"{uttID}\"\n",
    "    else:\n",
    "        spk2utt[speaker] += f\" {uttID}\"\n",
    "\n",
    "# Save files\n",
    "transcription.sort().save( os.path.join(new_path_dataset, \"text\") )\n",
    "utt2spk.save( os.path.join(new_path_dataset, \"utt2spk\") )\n",
    "spk2utt.save( os.path.join(new_path_dataset, \"spk2utt\") )  \n",
    "wavScp.sort().save(os.path.join(new_path_dataset,\"wav.scp\") )\n",
    "\n",
    "#Display\n",
    "#display(HTML(df[0:10].to_html()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31389381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spa1f001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SRC</th>\n",
       "      <th>LBO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SPA1F001</td>\n",
       "      <td>con estoico respeto a la justicia adyacente guardó sus flechas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPA1F002</td>\n",
       "      <td>fue inyectado en el abdomen y en una pierna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SPA1F003</td>\n",
       "      <td>la tensión volvió a aumentar el domingo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SPA1F004</td>\n",
       "      <td>cuando todavía eran baratos el vodka y el caviar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SPA1F005</td>\n",
       "      <td>sino el país mental de un patriota enloquecido</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SPA1F006</td>\n",
       "      <td>abría sus puertas a estos flacos alumnos afroamericanos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SPA1F007</td>\n",
       "      <td>palestina seguía en estado paupérrimo después de su duunvirato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SPA1F008</td>\n",
       "      <td>el presidente de la federación portuguesa de fútbol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SPA1F009</td>\n",
       "      <td>bajo los efectos de la hipnosis tocaba la guzla perfectamente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SPA1F010</td>\n",
       "      <td>el oftalmólogo dijo que le hizo entrar en un proceso subfebril</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create text file with transcriptions+accents\n",
    "# Preparing data structure for ExKaldi (WAV file, text file)\n",
    "\n",
    "# Prepare ExKaldi files\n",
    "transcription = exkaldi.Transcription(name=\"trans\")\n",
    "\n",
    "# Path\n",
    "path_info = ROOT_FOLDERS+'index_sp/contents.lst'\n",
    "\n",
    "# Read dataset transcriptions\n",
    "X = pd.read_csv(path_info, sep=\"\\t\", header=0, usecols=[0,1,7], encoding='latin-1')\n",
    "\n",
    "# Remove all rows with Utterance value missing\n",
    "X.dropna(subset = ['LBO'], inplace=True)\n",
    "\n",
    "# PREPROCESS TRANSCRIPTIONS\n",
    "# Drop all containing digits:\n",
    "X = X.drop(X[X.LBO.str.contains(r'\\d+')].index)\n",
    "# Label to lower case\n",
    "X['LBO'] = X['LBO'].str.lower()\n",
    "# Remove punctuations:\n",
    "X['LBO'] = X['LBO'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "# Convert to plain text. Remove accents and any other symbols like ¨, æ, etc.\n",
    "#X['LBO'] = X['LBO'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "# Rewrite directory column\n",
    "X['DIR'] = X.DIR.str.replace('\\\\', '/')\n",
    "# Mix columns 1 & 2 into one\n",
    "X['DIR'] = '/home/marrakchi/TFM/TFM_env/Datasets'+ X['DIR'].str.lower() + '/' + X['SRC'].str.lower()\n",
    "\n",
    "# Remove SRC column\n",
    "#X = X.drop(columns=['SRC'])\n",
    "\n",
    "#Convert to dataframe format and drop extra columns\n",
    "df = pd.DataFrame(X)\n",
    "df = df.drop(columns=['DIR'])\n",
    "df = df.reset_index(drop=True) # Refrech indexes\n",
    "\n",
    "for num,row in enumerate (df.SRC):\n",
    "    uttID = row.lower()\n",
    "    if num == 0:\n",
    "        print(uttID)\n",
    "    # 1. text\n",
    "    transcription[uttID] = df.LBO[num]\n",
    "\n",
    "# Save files\n",
    "transcription.sort().save( os.path.join(new_path_dataset, \"text+accents\") )\n",
    "\n",
    "#Display\n",
    "display(HTML(df[0:10].to_html()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b699ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d66a8c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DIR</th>\n",
       "      <th>SRC</th>\n",
       "      <th>LBO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/marrakchi/TFM/TFM_env/Datasets/inter1sp/f/sessa001/spa1f001</td>\n",
       "      <td>SPA1F001</td>\n",
       "      <td>con estoico respeto a la justicia adyacente guardo sus flechas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/marrakchi/TFM/TFM_env/Datasets/inter1sp/f/sessa001/spa1f002</td>\n",
       "      <td>SPA1F002</td>\n",
       "      <td>fue inyectado en el abdomen y en una pierna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/marrakchi/TFM/TFM_env/Datasets/inter1sp/f/sessa001/spa1f003</td>\n",
       "      <td>SPA1F003</td>\n",
       "      <td>la tension volvio a aumentar el domingo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/marrakchi/TFM/TFM_env/Datasets/inter1sp/f/sessa001/spa1f004</td>\n",
       "      <td>SPA1F004</td>\n",
       "      <td>cuando todavia eran baratos el vodka y el caviar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/marrakchi/TFM/TFM_env/Datasets/inter1sp/f/sessa001/spa1f005</td>\n",
       "      <td>SPA1F005</td>\n",
       "      <td>sino el pais mental de un patriota enloquecido</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/marrakchi/TFM/TFM_env/Datasets/inter1sp/f/sessa001/spa1f006</td>\n",
       "      <td>SPA1F006</td>\n",
       "      <td>abria sus puertas a estos flacos alumnos afroamericanos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/marrakchi/TFM/TFM_env/Datasets/inter1sp/f/sessa001/spa1f007</td>\n",
       "      <td>SPA1F007</td>\n",
       "      <td>palestina seguia en estado pauperrimo despues de su duunvirato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/marrakchi/TFM/TFM_env/Datasets/inter1sp/f/sessa001/spa1f008</td>\n",
       "      <td>SPA1F008</td>\n",
       "      <td>el presidente de la federacion portuguesa de futbol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/marrakchi/TFM/TFM_env/Datasets/inter1sp/f/sessa001/spa1f009</td>\n",
       "      <td>SPA1F009</td>\n",
       "      <td>bajo los efectos de la hipnosis tocaba la guzla perfectamente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/home/marrakchi/TFM/TFM_env/Datasets/inter1sp/f/sessa001/spa1f010</td>\n",
       "      <td>SPA1F010</td>\n",
       "      <td>el oftalmologo dijo que le hizo entrar en un proceso subfebril</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Getting all the audio paths\n",
    "\n",
    "#Display\n",
    "display(HTML(X[0:10].to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d81cfb4",
   "metadata": {},
   "source": [
    "# Compute MFCC+CMVN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8242d570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/x-wav;base64,UklGRiQEAABXQVZFZm10IBAAAAABAAEAgD4AAAB9AAACABAAZGF0YQAEAABGAEEANgA+AD8APABKAEQAQwA4ADgAPgBAAD0ATwBQAD8ASgA4AEwAOgAyAEAAKwBDADwAQQBBADIAQgA3ADwAQwA9AEUAOgBIAD4AOgA2ADEARQAfADIATABGADkAMAA9AD4AOgA4AEgAOgA1AD4ARQA5ADAAOwAxADQAOQA7AEoAMAAyAEAAJgBOAEMAMgBIAC4APQA1ADcAQAAlAEQATQAzAEQARQA5ADoANQBAADwATgBSADoAQQA3ADkAOABCAD4ANQAyAC8AOAAtAEUAMgAyADwAMgA0ADwARAAyAEUAQQBIAD0AOABDADAAMABAAEQANgBFADUAQQA+ADkARAAxAD4AOQBGADsANAA+ADkARQAsADYAPQAzADoAMwA6ADQAMgBFADsAMQBBADsASQBHADUATABMAEQAQQBCAEEAPAA/ADoAQwBPAE8ARgBLAE8ATgBLAEcASgBAAFcAVQBUAGYATABQAEMASwBaAE4AVgBMAFcAUwBSAFkAUgBKAEMAWQBGAE0AXgBKAEsARwBWAE8ARgBYAFkATgBcAFwAUQBcAFQAWwBUAD4AUABeAGQAVgBNAGMAWgBIAEsASABNAFgATgBLAEQARABMAEkAUABHAEwAPwBCAEYASwBaAEIAVQBQAE0ATgA5AEgARgBHAEEAQwBPAEgAPQBEAEUASwBSAD8ASABGAD4AQgBCAD8AQgBNAEAAPABAAEUAQwBIAEAARABHADQASQA+AEEARQA6AEwAQwA9AEEARgA5ADsAPQAwADwANgBDAD8AKwBFAEMANgBCADcAOQBJADgARQAzAC8ATAAyADwAMgBAAEkAPQA+ACcAPwBBAEcARwBCADwANQBAADcARgBAAEIARwA/ADUALQA4ADsARAA3AD0AOgBCAEsAPABHADQARQArACwASQBHAFUAGwBGAFQAPQBEADIASQBBAEIAPgA+ADkAMwA2AD4AQgA3AFIAQwBEAFAALAAsAD0ANgAyAEIAOgA+AEcAOgAxACcAQgBJADwAOAA9AD8AMQA6ADwATABLADoAJwArAEEAPgBRAD8AQwA5ADoAUQBEAEYALQA3ADYAQgBAADwAQQAiADoAPQBIADkALwBGAEEAPAA3AD8AMQA3ADoAMgA0AEEASgAzADwAPwA6AD8AOAA1ADgAOQBHAEwANwBLAEMAMABCAEQAQgA8AD0APAA+ADwAMwBMAEsAPwBQAEEASQBAAEAATwBDAFYATABKAEoASgBNAEMAQwBBAEEAUABKAE8AVABIAFAAQABUAEEAQgBLAEQATAA3AFYATgBWAFEAPgBNAEMAQwBNAEsARABRAEkAVQBgAFYASQA/AFIAUwA/AE4A\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_path = new_path_dataset+'wav/spa1f131.wav'\n",
    "ipd.display.Audio(filename=samp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bd93bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<exkaldi.core.archive.BytesFeat object at 0x7f022c82abe0>\n",
      "<exkaldi.core.archive.BytesFeat object at 0x7f022c82aeb8>\n",
      "13\n",
      "513\n",
      "['spa1f001']\n",
      "['spa1f001']\n"
     ]
    }
   ],
   "source": [
    "# Compute MFCC\n",
    "feat = exkaldi.compute_mfcc(samp_path, name=\"mfcc\")\n",
    "print(feat)\n",
    "# Compute spectrogram\n",
    "# default args: frameWidth=25, frameShift=10, windowType='povey'\n",
    "feat2 = exkaldi.compute_spectrogram(samp_path, rate=22050, name=\"spectrogram\")\n",
    "print(feat2)\n",
    "# Feature dimensions\n",
    "print(feat.dim)\n",
    "print(feat2.dim)\n",
    "# Feature utterances\n",
    "print(feat.utts)\n",
    "print(feat2.utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5671cbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "28800\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "parallel = 1\n",
    "mfccConfig={\"--use-energy\":\"true\"}\n",
    "print(ExKaldiInfo.timeout)\n",
    "ExKaldiInfo.set_timeout(28800) # 1/2 hour\n",
    "print(ExKaldiInfo.timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f75b20ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate SER_Dataset raw MFCC feature done.\n"
     ]
    }
   ],
   "source": [
    "feat = exkaldi.compute_mfcc(\n",
    "                        os.path.join(new_path_dataset,\"wav.scp\"),\n",
    "                        config=mfccConfig\n",
    "                        #outFile=os.path.join(path_dataset,\"mfcc\",\"train\",\"raw_mfcc.ark\")\n",
    "                    )\n",
    "feat.save( os.path.join(new_path_dataset,\"mfcc\",\"raw_mfcc.ark\") )\n",
    "print(f\"Generate SER_Dataset raw MFCC feature done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75feb2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start CMVN\n",
      "Generate CMVN statistics done.\n",
      "Generate MFCC feature (applied CMVN) done.\n"
     ]
    }
   ],
   "source": [
    "# Compute CMVN\n",
    "print('Start CMVN')\n",
    "cmvn = exkaldi.compute_cmvn_stats(\n",
    "                                feat=feat,\n",
    "                                spk2utt=os.path.join(new_path_dataset,\"spk2utt\"),\n",
    "                            )\n",
    "cmvn.save( os.path.join(new_path_dataset,\"mfcc\",\"cmvn.ark\") )\n",
    "print(f\"Generate CMVN statistics done.\")\n",
    "# Apply CMVN\n",
    "feat = exkaldi.use_cmvn(\n",
    "                    feat=feat,\n",
    "                    cmvn=cmvn,\n",
    "                    utt2spk=os.path.join(new_path_dataset,\"utt2spk\"),\n",
    "                )\n",
    "feat.save(os.path.join(new_path_dataset,\"mfcc\",\"mfcc_cmvn.ark\"))\n",
    "print(f\"Generate MFCC feature (applied CMVN) done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e72d75",
   "metadata": {},
   "source": [
    "# Test a GMM-HMM model with fmllr feature.\n",
    "## --> Speaker Adapted Training (SAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d7f8b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "delta = 2 #(n-order to add to the feature)\n",
    "numIters = 35 # Training iterations\n",
    "maxIterInc = 25 # Final iteration of increasing gaussians\n",
    "realignIter = [10,20,30] # iteration to allign feature\n",
    "order = 5 # N-GRAM to use (min 1 | max 6)\n",
    "beam = 13 # Decode beam size\n",
    "latBeam = 6 # Lattice beam size\n",
    "acwt = 0.083333 # Acoustic model weight\n",
    "parallel = 1 # parallel process to compute feature of train dataset (min 1 | max 10)\n",
    "skipTrain = False # If True, skip training. Do decoding only\n",
    "splice = 3 # How many left-right frames to splice\n",
    "fmllrIter = [2,4,6,12] # The iteration to estimate fmllr matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6682f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "declare.is_file( os.path.join(path_dataset,\"train_sat\",\"final.mdl\") )\n",
    "declare.is_file( os.path.join(path_dataset,\"train_sat\",\"tree\") )\n",
    "\n",
    "# Load tree\n",
    "tree = exkaldi.load_tree( os.path.join(path_dataset,\"train_sat\",\"tree\") )\n",
    "# Load model\n",
    "hmm = exkaldi.load_hmm( os.path.join(path_dataset,\"train_sat\",\"final.mdl\") )\n",
    "HCLGfile=os.path.join(path_dataset,\"train_sat\",\"graph\",f\"HCLG.{order}.fst\")\n",
    "tansformMatFile=os.path.join(path_dataset,\"train_lda_mllt\",\"trans.mat\")\n",
    "outDir=os.path.join(new_path_dataset,\"train_sat\",f\"decode_{order}grams\") # --> New Dataset path\n",
    "\n",
    "# Load dictionary/lexicon\n",
    "lexicons = exkaldi.load_lex(os.path.join(path_dataset,\"dict\",\"lexicons.lex\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1013a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform features\n",
      "Feature type is lda+mllt\n",
      "Transform feature\n",
      "Decode the first time with original feature.\n",
      "Estimate the primary fMLLR transform matrix.\n",
      "Transform feature with primary matrix.\n",
      "Decode the second time with primary fmllr feature.\n",
      "Determinize secondary lattice.\n",
      "Estimate the secondary fMLLR transform matrix.\n",
      "Compose the primary and secondary transform matrix.\n",
      "Transform feature with final matrix.\n",
      "Rescore secondary lattice.\n",
      "Determinize secondary lattice.\n",
      "Generate lattice done.\n",
      "Generate reference text done.\n",
      "Now score:\n",
      "Penalty: 0.0, LMWT: 1, PER: 36.71%\n",
      "Penalty: 0.0, LMWT: 2, PER: 36.24%\n",
      "Penalty: 0.0, LMWT: 3, PER: 35.79%\n",
      "Penalty: 0.0, LMWT: 4, PER: 35.33%\n",
      "Penalty: 0.0, LMWT: 5, PER: 34.9%\n",
      "Penalty: 0.0, LMWT: 6, PER: 34.45%\n",
      "Penalty: 0.0, LMWT: 7, PER: 34.15%\n",
      "Penalty: 0.0, LMWT: 8, PER: 33.84%\n",
      "Penalty: 0.0, LMWT: 9, PER: 33.66%\n",
      "Penalty: 0.0, LMWT: 10, PER: 33.5%\n",
      "Penalty: 0.5, LMWT: 1, PER: 36.69%\n",
      "Penalty: 0.5, LMWT: 2, PER: 36.2%\n",
      "Penalty: 0.5, LMWT: 3, PER: 35.72%\n",
      "Penalty: 0.5, LMWT: 4, PER: 35.26%\n",
      "Penalty: 0.5, LMWT: 5, PER: 34.78%\n",
      "Penalty: 0.5, LMWT: 6, PER: 34.34%\n",
      "Penalty: 0.5, LMWT: 7, PER: 34.04%\n",
      "Penalty: 0.5, LMWT: 8, PER: 33.8%\n",
      "Penalty: 0.5, LMWT: 9, PER: 33.62%\n",
      "Penalty: 0.5, LMWT: 10, PER: 33.51%\n",
      "Penalty: 1.0, LMWT: 1, PER: 36.66%\n",
      "Penalty: 1.0, LMWT: 2, PER: 36.16%\n",
      "Penalty: 1.0, LMWT: 3, PER: 35.66%\n",
      "Penalty: 1.0, LMWT: 4, PER: 35.2%\n",
      "Penalty: 1.0, LMWT: 5, PER: 34.69%\n",
      "Penalty: 1.0, LMWT: 6, PER: 34.27%\n",
      "Penalty: 1.0, LMWT: 7, PER: 34.02%\n",
      "Penalty: 1.0, LMWT: 8, PER: 33.78%\n",
      "Penalty: 1.0, LMWT: 9, PER: 33.64%\n",
      "Penalty: 1.0, LMWT: 10, PER: 33.55%\n",
      "Score done. Save the best result.\n"
     ]
    }
   ],
   "source": [
    "#print(f\"Load test feature.\")\n",
    "#feat = exkaldi.load_feat(os.path.join(new_path_dataset,\"mfcc\",\"mfcc_cmvn.ark\"))\n",
    "\n",
    "print(f\"Transform features\")\n",
    "if tansformMatFile is None:\n",
    "    print(\"Feature type is delta. Add 2-order deltas.\")\n",
    "    feat = feat.add_delta(order=delta)\n",
    "    feat = feat.save(os.path.join(outDir,\"mfcc_cmvn_delta.ark\"),returnIndexTable=True)\n",
    "else:\n",
    "    print(\"Feature type is lda+mllt\")\n",
    "    feat = feat.splice(left=splice,right=splice)\n",
    "    print(\"Transform feature\")\n",
    "    feat = exkaldi.transform_feat(feat, tansformMatFile)\n",
    "    feat = feat.save(os.path.join(outDir,\"mfcc_cmvn_lda.ark\"),returnIndexTable=True)\n",
    "\n",
    "## 1. Estimate the primary transform matrix from alignment or lattice.\n",
    "## We estimate it from lattice, so we decode it firstly.\n",
    "print(\"Decode the first time with original feature.\")\n",
    "preLat = exkaldi.decode.wfst.gmm_decode(\n",
    "                                    feat, \n",
    "                                    hmm, \n",
    "                                    HCLGfile, \n",
    "                                    symbolTable=lexicons(\"words\"),\n",
    "                                    beam=10, \n",
    "                                    latBeam=6, \n",
    "                                    acwt=acwt,\n",
    "                                    maxActive=2000,\n",
    "                                )\n",
    "preLat.save(os.path.join(outDir,\"test_premary.lat\"))\n",
    "preLat\n",
    "\n",
    "print(\"Estimate the primary fMLLR transform matrix.\")\n",
    "preTransMatrix = exkaldi.hmm.estimate_fMLLR_matrix(\n",
    "                                    aliOrLat=preLat,\n",
    "                                    lexicons=lexicons,\n",
    "                                    aliHmm=hmm, \n",
    "                                    feat=feat,\n",
    "                                    adaHmm=None,\n",
    "                                    silenceWeight=0.01,\n",
    "                                    acwt=acwt,\n",
    "                                    spk2utt=os.path.join(new_path_dataset,\"spk2utt\"),\n",
    "                                )\n",
    "del preLat\n",
    "## 2. Transform feature. We will use new feature to estimate the secondary transform matrix from lattice.\n",
    "print(\"Transform feature with primary matrix.\")\n",
    "fmllrFeat = exkaldi.use_fmllr(\n",
    "                    feat,\n",
    "                    preTransMatrix,\n",
    "                    utt2spk=os.path.join(new_path_dataset,\"utt2spk\"),\n",
    "                )\n",
    "print(\"Decode the second time with primary fmllr feature.\")\n",
    "secLat = exkaldi.decode.wfst.gmm_decode(\n",
    "                                    fmllrFeat, \n",
    "                                    hmm, \n",
    "                                    HCLGfile, \n",
    "                                    symbolTable=lexicons(\"words\"),\n",
    "                                    beam=beam,\n",
    "                                    latBeam=latBeam,\n",
    "                                    acwt=acwt,\n",
    "                                    maxActive=7000,\n",
    "                                    config={\"--determinize-lattice\":\"false\"},\n",
    "                                )\n",
    "print(\"Determinize secondary lattice.\")\n",
    "thiLat = secLat.determinize(acwt=acwt, beam=4)\n",
    "print(\"Estimate the secondary fMLLR transform matrix.\")\n",
    "secTransMatrix = exkaldi.hmm.estimate_fMLLR_matrix(\n",
    "                                    aliOrLat=thiLat,\n",
    "                                    lexicons=lexicons,\n",
    "                                    aliHmm=hmm, \n",
    "                                    feat=fmllrFeat,\n",
    "                                    adaHmm=None,\n",
    "                                    silenceWeight=0.01,\n",
    "                                    acwt=acwt,\n",
    "                                    spk2utt=os.path.join(new_path_dataset,\"spk2utt\"),\n",
    "                                )\n",
    "del fmllrFeat\n",
    "del thiLat\n",
    "## 3. Compose the primary matrix and secondary matrix and get the final transform matrix.\n",
    "print(\"Compose the primary and secondary transform matrix.\")\n",
    "finalTransMatrix = exkaldi.hmm.compose_transform_matrixs(\n",
    "                                                    matA=preTransMatrix,\n",
    "                                                    matB=secTransMatrix,\n",
    "                                                    bIsAffine=True,\n",
    "                                                )\n",
    "finalTransMatrix.save(os.path.join(outDir,\"trans.ark\"))\n",
    "print(\"Transform feature with final matrix.\")\n",
    "## 4. Transform feature with the final transform matrix and use it to decode.\n",
    "## We directly use the lattice generated in the second step. The final lattice is obtained.\n",
    "finalFmllrFeat = exkaldi.use_fmllr(\n",
    "                                feat,\n",
    "                                finalTransMatrix,\n",
    "                                utt2spk=os.path.join(new_path_dataset,\"utt2spk\"),\n",
    "                            )\n",
    "del finalTransMatrix\n",
    "print(\"Rescore secondary lattice.\")\n",
    "lat = secLat.am_rescore(\n",
    "                    hmm=hmm,\n",
    "                    feat=finalFmllrFeat,\n",
    "                )\n",
    "print(\"Determinize secondary lattice.\")\n",
    "lat = lat.determinize(acwt=acwt, beam=6)\n",
    "lat.save(os.path.join(outDir,\"test.lat\"))\n",
    "#lat = exkaldi.load_lat(os.path.join(outDir,\"test.lat\"))\n",
    "print(\"Generate lattice done.\")\n",
    "\n",
    "# TEXTO DE REFERENCIA (fonemas)\n",
    "refText = exkaldi.load_transcription(os.path.join(new_path_dataset,\"text\"))\n",
    "# Transform transcriptions from words to phones\n",
    "word2phone = exkaldi.load_list_table(path_dataset+'word2phone.map')\n",
    "refText = refText.convert(word2phone, unkSymbol='<UNK>')\n",
    "print(\"Generate reference text done.\")\n",
    "refText.save(os.path.join(outDir,\"ref.txt\") )\n",
    "\n",
    "# Penalty: 0.0, LMWT: 10, PER: 22.74%\n",
    "\n",
    "print(\"Now score:\")\n",
    "bestWER = (1000, 0, 0)\n",
    "bestResult = None\n",
    "\n",
    "for penalty in [0., 0.5, 1.0]:\n",
    "    for LMWT in range(1, 11):\n",
    "        # Add penalty\n",
    "        newLat = lat.add_penalty(penalty)\n",
    "\n",
    "        # RESULTADOS PREDECIDOS (int)\n",
    "        # Get 1-best result (phone-level)\n",
    "        result = newLat.get_1best(lexicons(\"phones\"), hmm, lmwt=LMWT, acwt=1, phoneLevel=True)\n",
    "\n",
    "        # RESULTADOS PREDECIDOS (fonemas)\n",
    "        # Transform from int value format to text format\n",
    "        result = exkaldi.hmm.transcription_from_int(result, lexicons(\"phones\"))\n",
    "\n",
    "        # Compute PER\n",
    "        score = exkaldi.decode.score.wer(ref=refText, hyp=result, mode=\"present\")\n",
    "        \n",
    "        if score.WER < bestWER[0]:\n",
    "            bestResult = result\n",
    "            bestWER = (score.WER, penalty, LMWT)\n",
    "        print(f\"Penalty: {penalty}, LMWT: {LMWT}, PER: {score.WER}%\")\n",
    "        \n",
    "\n",
    "print(\"Score done. Save the best result.\")\n",
    "bestResult.save(os.path.join(outDir, \"hyp.txt\") )\n",
    "\n",
    "with open(os.path.join(outDir,\"best_PER\"),\"w\") as fw:\n",
    "    fw.write( f\"PER {bestWER[0]}, penalty {bestWER[1]}, LMWT {bestWER[2]}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6677e7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spa1f001': 'con estoico respeto a la justicia adyacente guardó sus flechas'}\n",
      "Now score:\n",
      "{'spa1f001': '19428 36960 75793 51942 51543 24779 43699 83006 40444'}\n",
      "{'spa1f001': 'con estoico respeto la justicia decente guardo sus flechas'}\n",
      "Score(WER=30.0, words=10, insErr=0, delErr=1, subErr=2, SER=100.0, sentences=1, wrongSentences=1, missedSentences=0)\n"
     ]
    }
   ],
   "source": [
    "lat = exkaldi.load_lat(os.path.join(outDir,\"test.lat\"))\n",
    "\n",
    "# TEXTO DE REFERENCIA (palabras)\n",
    "refText = exkaldi.load_transcription(os.path.join(new_path_dataset,\"text+accents\"))\n",
    "print(refText.subset(nHead=1))\n",
    "#refText.save(os.path.join(outDir,\"ref_words_accents.txt\"))\n",
    "\n",
    "print(\"Now score:\")\n",
    "#bestWER = (1000, 0, 0)\n",
    "#bestResult = None\n",
    "\n",
    "penalty = 1\n",
    "LMWT = 10\n",
    "\n",
    "#for penalty in [0., 0.5, 1.0]:\n",
    "#    for LMWT in range(1, 11):\n",
    "# Add penalty\n",
    "#print(penalty)\n",
    "#print(LMWT)\n",
    "newLat = lat.add_penalty(penalty)\n",
    "#print(newLat)\n",
    "# RESULTADOS PREDECIDOS (int)\n",
    "# Get 1-best result (phone-level)\n",
    "result = newLat.get_1best(lexicons(\"words\"), hmm, lmwt=LMWT, acwt=1, phoneLevel=False)\n",
    "print(result.subset(nHead=1))\n",
    "\n",
    "# RESULTADOS PREDECIDOS (palabras)\n",
    "# Transform from int value format to text format\n",
    "result = exkaldi.hmm.transcription_from_int(result, lexicons(\"words\"))\n",
    "print(result.subset(nHead=1))\n",
    "\n",
    "# Compute PER\n",
    "score = exkaldi.decode.score.wer(ref=refText.subset(nHead=1), hyp=result.subset(nHead=1), mode=\"present\")\n",
    "print(score)\n",
    "\n",
    "#if score.WER < bestWER[0]:\n",
    "#    bestResult = result\n",
    "#    bestWER = (score.WER, penalty, LMWT)\n",
    "#print(f\"Penalty: {penalty}, LMWT: {LMWT}, WER: {score.WER}%\")\n",
    "        \n",
    "#print(\"Score done. Save the best result.\")\n",
    "#bestResult.save(os.path.join(outDir, \"hyp_words.txt\") )\n",
    "#with open(os.path.join(outDir,\"best_WER\"),\"w\") as fw:\n",
    "#    fw.write( f\"WER {bestWER[0]}, penalty {bestWER[1]}, LMWT {bestWER[2]}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6fe69cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.63\n"
     ]
    }
   ],
   "source": [
    "print(score.WER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f5ad346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now score:\n",
      "Penalty: 0.0, LMWT: 1, WER: 69.56%\n",
      "Penalty: 0.0, LMWT: 2, WER: 67.56%\n",
      "Penalty: 0.0, LMWT: 3, WER: 65.5%\n",
      "Penalty: 0.0, LMWT: 4, WER: 63.39%\n",
      "Penalty: 0.0, LMWT: 5, WER: 61.22%\n",
      "Penalty: 0.0, LMWT: 6, WER: 58.84%\n",
      "Penalty: 0.0, LMWT: 7, WER: 56.86%\n",
      "Penalty: 0.0, LMWT: 8, WER: 54.78%\n",
      "Penalty: 0.0, LMWT: 9, WER: 53.02%\n",
      "Penalty: 0.0, LMWT: 10, WER: 51.72%\n",
      "Penalty: 0.5, LMWT: 1, WER: 69.29%\n",
      "Penalty: 0.5, LMWT: 2, WER: 67.07%\n",
      "Penalty: 0.5, LMWT: 3, WER: 64.83%\n",
      "Penalty: 0.5, LMWT: 4, WER: 62.56%\n",
      "Penalty: 0.5, LMWT: 5, WER: 60.05%\n",
      "Penalty: 0.5, LMWT: 6, WER: 57.67%\n",
      "Penalty: 0.5, LMWT: 7, WER: 55.56%\n",
      "Penalty: 0.5, LMWT: 8, WER: 53.53%\n",
      "Penalty: 0.5, LMWT: 9, WER: 51.98%\n",
      "Penalty: 0.5, LMWT: 10, WER: 50.65%\n",
      "Penalty: 1.0, LMWT: 1, WER: 69.05%\n",
      "Penalty: 1.0, LMWT: 2, WER: 66.72%\n",
      "Penalty: 1.0, LMWT: 3, WER: 64.21%\n",
      "Penalty: 1.0, LMWT: 4, WER: 61.78%\n",
      "Penalty: 1.0, LMWT: 5, WER: 59.11%\n",
      "Penalty: 1.0, LMWT: 6, WER: 56.72%\n",
      "Penalty: 1.0, LMWT: 7, WER: 54.6%\n",
      "Penalty: 1.0, LMWT: 8, WER: 52.58%\n",
      "Penalty: 1.0, LMWT: 9, WER: 51.16%\n",
      "Penalty: 1.0, LMWT: 10, WER: 49.99%\n",
      "Score done. Save the best result.\n"
     ]
    }
   ],
   "source": [
    "#lat = exkaldi.load_lat(os.path.join(outDir,\"test.lat\"))\n",
    "\n",
    "# TEXTO DE REFERENCIA (palabras)\n",
    "refText = exkaldi.load_transcription(os.path.join(new_path_dataset,\"text\"))\n",
    "refText.save(os.path.join(outDir,\"ref_words.txt\"))\n",
    "\n",
    "print(\"Now score:\")\n",
    "bestWER = (1000, 0, 0)\n",
    "bestResult = None\n",
    "\n",
    "#penalty = 1\n",
    "#LMWT = 10\n",
    "\n",
    "for penalty in [0., 0.5, 1.0]:\n",
    "    for LMWT in range(1, 11):\n",
    "        # Add penalty\n",
    "        #print(penalty)\n",
    "        #print(LMWT)\n",
    "        newLat = lat.add_penalty(penalty)\n",
    "        #print(newLat)\n",
    "        # RESULTADOS PREDECIDOS (int)\n",
    "        # Get 1-best result (phone-level)\n",
    "        result = newLat.get_1best(lexicons(\"words\"), hmm, lmwt=LMWT, acwt=1, phoneLevel=False)\n",
    "        #print(result.subset(nHead=1))\n",
    "\n",
    "        # RESULTADOS PREDECIDOS (palabras)\n",
    "        # Transform from int value format to text format\n",
    "        result = exkaldi.hmm.transcription_from_int(result, lexicons(\"words\"))\n",
    "        #print(result.subset(nHead=1))\n",
    "\n",
    "        # Compute PER\n",
    "        score = exkaldi.decode.score.wer(ref=refText, hyp=result, mode=\"present\")\n",
    "        #print(score)\n",
    "\n",
    "        if score.WER < bestWER[0]:\n",
    "            bestResult = result\n",
    "            bestWER = (score.WER, penalty, LMWT)\n",
    "        print(f\"Penalty: {penalty}, LMWT: {LMWT}, WER: {score.WER}%\")\n",
    "        \n",
    "print(\"Score done. Save the best result.\")\n",
    "bestResult.save(os.path.join(outDir, \"hyp_words.txt\") )\n",
    "with open(os.path.join(outDir,\"best_WER\"),\"w\") as fw:\n",
    "    fw.write( f\"WER {bestWER[0]}, penalty {bestWER[1]}, LMWT {bestWER[2]}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412b78e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
