{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Primer intento: MONOMODO\n",
    "--> MFCC\n",
    "--> KENLM (5 grams)\n",
    "--> parallel args = None\n",
    "--> mfccConfig with energy\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LXoRd3n8nUdl"
   },
   "outputs": [],
   "source": [
    "path_dataset ='/home/marrakchi/TFM/TFM_env/Datasets/ExKaldiASRDatasetFormat/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "2PXQOtUznUdx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Kaldi root directory was not found in system PATH. You can appoint it:\n",
      "exkaldi.info.reset_kaldi_root( yourPath )\n",
      "If not, ERROR will occur when implementing some core functions.\n"
     ]
    }
   ],
   "source": [
    "# Systems Libraries\n",
    "import os\n",
    "import time\n",
    "\n",
    "# For audio processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython as ipd\n",
    "from IPython.display import display, HTML # For displaying tables\n",
    "\n",
    "#from torchsummary import summary\n",
    "\n",
    "# For data processing\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "#import sox\n",
    "\n",
    "# For Neural networks\n",
    "import shutil,socket\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import pickle as pkl\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# For text processing\n",
    "import string\n",
    "from datetime import datetime\n",
    "\n",
    "# Import exkaldi package\n",
    "import exkaldi\n",
    "from exkaldi import declare\n",
    "exkaldi.info.reset_kaldi_root('/kaldi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExKaldiInfo(version='1.3.5.4', major='1', minor='3', patch='5', upload='4')\n",
      "Kaldi(version='5.5', major='5', minor='5')\n",
      "/kaldi\n"
     ]
    }
   ],
   "source": [
    "# ExKaldi Configuration\n",
    "ExKaldiInfo = exkaldi.info\n",
    "print(ExKaldiInfo)\n",
    "print(ExKaldiInfo.KALDI)\n",
    "print(ExKaldiInfo.KALDI_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Ensuring that GPU are detected/available and ready to use\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123956\n"
     ]
    }
   ],
   "source": [
    "# Read txt file as a list\n",
    "mi_path = path_dataset+\"wav_names.txt\"\n",
    "file = open(mi_path, 'r') \n",
    "text = file.read() \n",
    "wav_list = text.split(\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(len(wav_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0000df16-47ea-428f-8367-df2ce365d5c4.wav', '000127f7-2705-4135-b16c-04daaa80e88d.wav', '0001d4a5-e7f1-42a4-b71f-74bd1549f2d4.wav', '000275c9-1aa3-4112-81df-0883a86d3fb7.wav', '0002a149-56cf-4ce0-b2c8-506a53b47012.wav']\n"
     ]
    }
   ],
   "source": [
    "wav_list.sort()\n",
    "print(wav_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<exkaldi.decode.graph.LexiconBank object at 0x7ffa6bccce48>\n"
     ]
    }
   ],
   "source": [
    "# Load lexicon\n",
    "lexicons = exkaldi.load_lex(os.path.join(path_dataset, \"dict\", \"lexicons.lex\"))\n",
    "print(lexicons)\n",
    "# Load transcriptions\n",
    "trans = exkaldi.load_transcription(os.path.join(path_dataset,\"text\"))\n",
    "trainTrans = exkaldi.load_transcription(os.path.join(path_dataset,'data/train',\"text\"))\n",
    "valTrans = exkaldi.load_transcription(os.path.join(path_dataset,'data/val',\"text\"))\n",
    "testTrans = exkaldi.load_transcription(os.path.join(path_dataset,'data/test',\"text\"))\n",
    "# Load wavScp\n",
    "wavScp = exkaldi.load_list_table(os.path.join(path_dataset,\"wav.scp\") )\n",
    "wavScp_train = exkaldi.load_list_table(os.path.join(path_dataset,'data/train',\"wav.scp\") )\n",
    "wavScp_val = exkaldi.load_list_table(os.path.join(path_dataset,'data/val',\"wav.scp\") )\n",
    "wavScp_test = exkaldi.load_list_table(os.path.join(path_dataset,'data/test',\"wav.scp\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "parallel = 1\n",
    "mfccConfig={\"--use-energy\":\"true\"}\n",
    "#mfccConfig={\"--use-energy\":\"false\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n",
      "28800\n"
     ]
    }
   ],
   "source": [
    "print(ExKaldiInfo.timeout)\n",
    "ExKaldiInfo.set_timeout(28800) # 1/2 hour\n",
    "print(ExKaldiInfo.timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate TRAIN raw MFCC feature done.\n"
     ]
    }
   ],
   "source": [
    "# Training without parallel processes\n",
    "#------------------- Train -------------------------------------\n",
    "\"\"\"wavFiles = exkaldi.utils.split_txt_file(\n",
    "                                os.path.join(path_dataset,\"data\",\"train\",\"wav.scp\"), \n",
    "                                chunks=parallel,\n",
    "                            )\n",
    "\"\"\"\n",
    "feat = exkaldi.compute_mfcc(\n",
    "                        os.path.join(path_dataset,\"data\",\"train\",\"wav.scp\"),\n",
    "                        config=mfccConfig\n",
    "                        #outFile=os.path.join(path_dataset,\"mfcc\",\"train\",\"raw_mfcc.ark\")\n",
    "                    )\n",
    "#feat = exkaldi.merge_archives(feats) # Merge multiple archives to one\n",
    "#feat = feat.fetch(arkType='feat')\n",
    "feat.save( os.path.join(path_dataset,\"mfcc\",\"train\",\"raw_mfcc.ark\") )\n",
    "print(f\"Generate TRAIN raw MFCC feature done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate TRAIN raw MFCC feature done.\n",
      "Generate VAL raw MFCC feature done.\n",
      "Generate TEST raw MFCC feature done.\n"
     ]
    }
   ],
   "source": [
    "# 1. compute feature (MFCC)\n",
    "#------------------- Train -------------------------------------\n",
    "wavFiles = exkaldi.utils.split_txt_file(\n",
    "                                os.path.join(path_dataset,\"data\",\"train\",\"wav.scp\"), \n",
    "                                chunks=parallel,\n",
    "                            )\n",
    "feats = exkaldi.compute_mfcc(\n",
    "                        wavFiles, \n",
    "                        config=mfccConfig,\n",
    "                        outFile=os.path.join(path_dataset,\"mfcc\",\"train\",\"raw_mfcc.ark\")\n",
    "                    )\n",
    "feat = exkaldi.merge_archives(feats) # Merge multiple archives to one\n",
    "feat = feat.fetch(arkType='feat')\n",
    "feat.save( os.path.join(path_dataset,\"mfcc\",\"train\",\"raw_mfcc.ark\") )\n",
    "print(f\"Generate TRAIN raw MFCC feature done.\")\n",
    "\n",
    "#-------------------- Val --------------------------------------\n",
    "feat2 = exkaldi.compute_mfcc(\n",
    "                        os.path.join(path_dataset,\"data\",'val',\"wav.scp\"), \n",
    "                        config=mfccConfig,\n",
    "                    )\n",
    "feat2.save( os.path.join(path_dataset,\"mfcc\",\"val\",\"raw_mfcc.ark\") )\n",
    "print(f\"Generate VAL raw MFCC feature done.\")\n",
    "\n",
    "#-------------------- Test -------------------------------------\n",
    "feat3 = exkaldi.compute_mfcc(\n",
    "                        os.path.join(path_dataset,\"data\",\"test\",\"wav.scp\"), \n",
    "                        config=mfccConfig,\n",
    "                    )\n",
    "feat3.save( os.path.join(path_dataset,\"mfcc\",\"test\",\"raw_mfcc.ark\") )\n",
    "\n",
    "print(f\"Generate TEST raw MFCC feature done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train feat loaded\n",
      "Start CMVN\n",
      "Generate CMVN statistics for train done.\n",
      "Generate MFCC feature (applied CMVN) for train done.\n",
      "val feat loaded\n",
      "Start CMVN\n",
      "Generate CMVN statistics for val done.\n",
      "Generate MFCC feature (applied CMVN) for val done.\n",
      "test feat loaded\n",
      "Start CMVN\n",
      "Generate CMVN statistics for test done.\n",
      "Generate MFCC feature (applied CMVN) for test done.\n"
     ]
    }
   ],
   "source": [
    "for Name in [\"train\", \"val\", \"test\"]:\n",
    "    feat = exkaldi.load_index_table(os.path.join(path_dataset,\"mfcc\",Name,\"raw_mfcc.ark\"))\n",
    "    print(Name+' feat loaded')\n",
    "    # Compute CMVN\n",
    "    print('Start CMVN')\n",
    "    cmvn = exkaldi.compute_cmvn_stats(\n",
    "                                    feat=feat,\n",
    "                                    spk2utt=os.path.join(path_dataset,\"data\",Name,\"spk2utt\"),\n",
    "                                )\n",
    "    cmvn.save( os.path.join(path_dataset,\"mfcc\",Name,\"cmvn.ark\") )\n",
    "    print(f\"Generate CMVN statistics for \"+ Name +\" done.\")\n",
    "    # Apply CMVN\n",
    "    feat = exkaldi.use_cmvn(\n",
    "                        feat=feat,\n",
    "                        cmvn=cmvn,\n",
    "                        utt2spk=os.path.join(path_dataset,\"data\",Name,\"utt2spk\"),\n",
    "                    )\n",
    "    feat.save(os.path.join(path_dataset,\"mfcc\",Name,\"mfcc_cmvn.ark\"))\n",
    "    print(f\"Generate MFCC feature (applied CMVN) for \"+ Name +\" done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_WFST_graph(outDir, hmm, tree):\n",
    "\n",
    "    print(\"Start to make WFST graph.\")\n",
    "    exkaldi.utils.make_dependent_dirs(outDir, pathIsFile=False)\n",
    "    print(f\"Load lexicon bank.\")\n",
    "    lexicons = exkaldi.load_lex(os.path.join(path_dataset,\"dict\",\"lexicons.lex\"))\n",
    "    \n",
    "    # iLabel file will be generated in this step.\n",
    "    _, ilabelFile = exkaldi.decode.graph.compose_CLG(\n",
    "                                            lexicons,\n",
    "                                            tree,\n",
    "                                            os.path.join(path_dataset,\"lm\",f\"LG.{order}.fst\"),\n",
    "                                            outFile=os.path.join(outDir,f\"CLG.{order}.fst\"),\n",
    "                                        )\n",
    "    print(f\"Generate CLG fst done.\")\n",
    "    exkaldi.decode.graph.compose_HCLG(\n",
    "                                    hmm,\n",
    "                                    tree,\n",
    "                                    CLGFile=os.path.join(outDir,f\"CLG.{order}.fst\"),\n",
    "                                    iLabelFile=ilabelFile,\n",
    "                                    outFile=os.path.join(outDir,f\"HCLG.{order}.fst\"),\n",
    "                                )\n",
    "    print(f\"Compose HCLG fst done.\")\n",
    "\n",
    "def GMM_decode_mfcc_and_score(outDir, hmm, HCLGfile, tansformMatFile=None):\n",
    "\n",
    "    exkaldi.utils.make_dependent_dirs(outDir, pathIsFile=False)\n",
    "\n",
    "    lexicons = exkaldi.load_lex(os.path.join(path_dataset,\"dict\",\"lexicons.lex\"))\n",
    "    print(f\"Load test feature.\")\n",
    "    featFile = os.path.join(path_dataset,\"mfcc\",\"test\",\"mfcc_cmvn.ark\")\n",
    "    feat = exkaldi.load_feat(featFile)\n",
    "    if tansformMatFile is None:\n",
    "        print(f\"Feature type is delta. Add {delta}-order deltas.\")\n",
    "        feat = feat.add_delta(order=delta)\n",
    "    else:\n",
    "        print(f\"Feature type is lda+mllt. Transform feature.\")\n",
    "        feat = feat.splice(left=splice,right=splice)\n",
    "        feat = exkaldi.transform_feat(feat, tansformMatFile)\n",
    "    \n",
    "    if parallel > 1:\n",
    "        feat = feat.subset(chunks=parallel)\n",
    "\n",
    "    print(\"Start to decode\")\n",
    "    st = time.time()\n",
    "    lat = exkaldi.decode.wfst.gmm_decode(\n",
    "                                    feat, \n",
    "                                    hmm, \n",
    "                                    HCLGfile, \n",
    "                                    symbolTable=lexicons(\"words\"),\n",
    "                                    beam=beam, \n",
    "                                    latBeam=latBeam, \n",
    "                                    acwt=acwt,\n",
    "                                    outFile=os.path.join(outDir,\"test.lat\"),\n",
    "                                )\n",
    "    print(\"Decode time cost: \",\"%.2f\"%(time.time()-st),\"seconds\")\n",
    "    if isinstance(lat,list):\n",
    "        lat = exkaldi.merge_archives(lat)\n",
    "\n",
    "    print(f\"Generate lattice done.\")\n",
    "    \n",
    "    refText = exkaldi.load_transcription(os.path.join(path_dataset,\"data\",\"test\",\"text\"))\n",
    "    # Transform transcriptions from words to phones\n",
    "    word2phone = exkaldi.load_list_table(path_dataset+'word2phone.map')\n",
    "    refText = refText.convert(word2phone, unkSymbol='<UNK>')\n",
    "\n",
    "    print(\"Now score:\")\n",
    "    bestWER = (1000, 0, 0)\n",
    "    bestResult = None\n",
    "    for penalty in [0., 0.5, 1.0]:\n",
    "        for LMWT in range(1, 11):\n",
    "            # Add penalty\n",
    "            newLat = lat.add_penalty(penalty)\n",
    "            # Get 1-best result (phone-level)\n",
    "            result = newLat.get_1best(lexicons(\"phones\"), hmm, lmwt=LMWT, acwt=1, phoneLevel=True)\n",
    "            # Transform from int value format to text format\n",
    "            result = exkaldi.hmm.transcription_from_int(result, lexicons(\"phones\"))\n",
    "            # Compute PER\n",
    "            score = exkaldi.decode.score.wer(ref=refText, hyp=result, mode=\"present\")\n",
    "            if score.WER < bestWER[0]:\n",
    "                bestResult = result\n",
    "                bestWER = (score.WER, penalty, LMWT)\n",
    "            print(f\"Penalty: {penalty}, LMWT: {LMWT}, PER: {score.WER}%\")\n",
    "\n",
    "    print(\"Score done. Saved the best result.\")\n",
    "    bestResult.save(os.path.join(outDir, \"hyp.txt\") )\n",
    "    with open(os.path.join(outDir,\"best_PER\"),\"w\") as fw:\n",
    "        fw.write( f\"PER {bestWER[0]}, penalty {bestWER[1]}, LMWT {bestWER[2]}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_decode_fmllr_and_score(outDir, hmm, HCLGfile, tansformMatFile=None):\n",
    "    exkaldi.utils.make_dependent_dirs(outDir, pathIsFile=False)\n",
    "    lexicons = exkaldi.load_lex(os.path.join(path_dataset,\"dict\",\"lexicons.lex\"))\n",
    "    print(f\"Load test feature.\")\n",
    "    featFile = os.path.join(path_dataset,\"mfcc\",\"test\",\"mfcc_cmvn.ark\")\n",
    "    feat = exkaldi.load_feat(featFile)\n",
    "    if tansformMatFile is None:\n",
    "        print(\"Feature type is delta. Add 2-order deltas.\")\n",
    "        feat = feat.add_delta(order=delta)\n",
    "        feat = feat.save(os.path.join(outDir,\"test_mfcc_cmvn_delta.ark\"),returnIndexTable=True)\n",
    "    else:\n",
    "        print(\"Feature type is lda+mllt\")\n",
    "        feat = feat.splice(left=splice,right=splice)\n",
    "        print(\"Transform feature\")\n",
    "        feat = exkaldi.transform_feat(feat, tansformMatFile)\n",
    "        feat = feat.save(os.path.join(outDir,\"test_mfcc_cmvn_lda.ark\"),returnIndexTable=True)\n",
    "\n",
    "    ## 1. Estimate the primary transform matrix from alignment or lattice.\n",
    "    ## We estimate it from lattice, so we decode it firstly.\n",
    "    print(\"Decode the first time with original feature.\")\n",
    "    preLat = exkaldi.decode.wfst.gmm_decode(\n",
    "                                        feat, \n",
    "                                        hmm, \n",
    "                                        HCLGfile, \n",
    "                                        symbolTable=lexicons(\"words\"),\n",
    "                                        beam=10, \n",
    "                                        latBeam=6, \n",
    "                                        acwt=acwt,\n",
    "                                        maxActive=2000,\n",
    "                                    )\n",
    "    preLat.save(os.path.join(outDir,\"test_premary.lat\"))\n",
    "\n",
    "    print(\"Estimate the primary fMLLR transform matrix.\")\n",
    "    preTransMatrix = exkaldi.hmm.estimate_fMLLR_matrix(\n",
    "                                        aliOrLat=preLat,\n",
    "                                        lexicons=lexicons,\n",
    "                                        aliHmm=hmm, \n",
    "                                        feat=feat,\n",
    "                                        adaHmm=None,\n",
    "                                        silenceWeight=0.01,\n",
    "                                        acwt=acwt,\n",
    "                                        spk2utt=os.path.join(path_dataset,\"data\",\"test\",\"spk2utt\"),\n",
    "                                    )\n",
    "    del preLat\n",
    "    ## 2. Transform feature. We will use new feature to estimate the secondary transform matrix from lattice.\n",
    "    print(\"Transform feature with primary matrix.\")\n",
    "    fmllrFeat = exkaldi.use_fmllr(\n",
    "                        feat,\n",
    "                        preTransMatrix,\n",
    "                        utt2spk=os.path.join(path_dataset,\"data\",\"test\",\"utt2spk\"),\n",
    "                    )\n",
    "    print(\"Decode the second time with primary fmllr feature.\")\n",
    "    secLat = exkaldi.decode.wfst.gmm_decode(\n",
    "                                        fmllrFeat, \n",
    "                                        hmm, \n",
    "                                        HCLGfile, \n",
    "                                        symbolTable=lexicons(\"words\"),\n",
    "                                        beam=beam,\n",
    "                                        latBeam=latBeam,\n",
    "                                        acwt=acwt,\n",
    "                                        maxActive=7000,\n",
    "                                        config={\"--determinize-lattice\":\"false\"},\n",
    "                                    )\n",
    "    print(\"Determinize secondary lattice.\")\n",
    "    thiLat = secLat.determinize(acwt=acwt, beam=4)\n",
    "    print(\"Estimate the secondary fMLLR transform matrix.\")\n",
    "    secTransMatrix = exkaldi.hmm.estimate_fMLLR_matrix(\n",
    "                                        aliOrLat=thiLat,\n",
    "                                        lexicons=lexicons,\n",
    "                                        aliHmm=hmm, \n",
    "                                        feat=fmllrFeat,\n",
    "                                        adaHmm=None,\n",
    "                                        silenceWeight=0.01,\n",
    "                                        acwt=acwt,\n",
    "                                        spk2utt=os.path.join(path_dataset,\"data\",\"test\",\"spk2utt\"),\n",
    "                                    )\n",
    "    del fmllrFeat\n",
    "    del thiLat\n",
    "    ## 3. Compose the primary matrix and secondary matrix and get the final transform matrix.\n",
    "    print(\"Compose the primary and secondary transform matrix.\")\n",
    "    finalTransMatrix = exkaldi.hmm.compose_transform_matrixs(\n",
    "                                                        matA=preTransMatrix,\n",
    "                                                        matB=secTransMatrix,\n",
    "                                                        bIsAffine=True,\n",
    "                                                    )\n",
    "    finalTransMatrix.save(os.path.join(outDir,\"trans.ark\"))\n",
    "    print(\"Transform feature with final matrix.\")\n",
    "    ## 4. Transform feature with the final transform matrix and use it to decode.\n",
    "    ## We directly use the lattice generated in the second step. The final lattice is obtained.\n",
    "    finalFmllrFeat = exkaldi.use_fmllr(\n",
    "                                    feat,\n",
    "                                    finalTransMatrix,\n",
    "                                    utt2spk=os.path.join(path_dataset,\"data\",\"test\",\"utt2spk\"),\n",
    "                                )\n",
    "    del finalTransMatrix\n",
    "    print(\"Rescore secondary lattice.\")\n",
    "    lat = secLat.am_rescore(\n",
    "                        hmm=hmm,\n",
    "                        feat=finalFmllrFeat,\n",
    "                    )\n",
    "    print(\"Determinize secondary lattice.\")\n",
    "    lat = lat.determinize(acwt=acwt, beam=6)\n",
    "    lat.save(os.path.join(outDir,\"test.lat\"))\n",
    "    print(\"Generate lattice done.\")\n",
    "\n",
    "    #phoneMapFile = os.path.join(path_dataset,\"dict\",\"phones.48_to_39.map\")\n",
    "    #phoneMap = exkaldi.load_list_table(phoneMapFile,name=\"48-39\")\n",
    "    #refText = exkaldi.load_transcription(os.path.join(args.expDir,\"data\",\"test\",\"text\")).convert(phoneMap)\n",
    "    \n",
    "    refText = exkaldi.load_transcription(os.path.join(path_dataset,\"data\",\"test\",\"text\"))\n",
    "    # Transform transcriptions from words to phones\n",
    "    word2phone = exkaldi.load_list_table(path_dataset+'word2phone.map')\n",
    "    refText = refText.convert(word2phone, unkSymbol='<UNK>')\n",
    "    \n",
    "    refText.save(os.path.join(outDir,\"ref.txt\") )\n",
    "    print(\"Generate reference text done.\")\n",
    "\n",
    "    print(\"Now score:\")\n",
    "    bestWER = (1000, 0, 0)\n",
    "    bestResult = None\n",
    "    for penalty in [0., 0.5, 1.0]:\n",
    "        for LMWT in range(1, 11):\n",
    "            # Add penalty\n",
    "            newLat = lat.add_penalty(penalty)\n",
    "            # Get 1-best result (phone-level)\n",
    "            result = newLat.get_1best(lexicons(\"phones\"), hmm, lmwt=LMWT, acwt=1, phoneLevel=True)\n",
    "            # Transform from int value format to text format\n",
    "            result = exkaldi.hmm.transcription_from_int(result, lexicons(\"phones\"))\n",
    "            # Transform 48-phones to 39-phones\n",
    "            #result = result.convert(phoneMap, None)\n",
    "            # Compute PER\n",
    "            score = exkaldi.decode.score.wer(ref=refText, hyp=result, mode=\"present\")\n",
    "            if score.WER < bestWER[0]:\n",
    "                bestResult = result\n",
    "                bestWER = (score.WER, penalty, LMWT)\n",
    "            print(f\"Penalty: {penalty}, LMWT: {LMWT}, PER: {score.WER}%\")\n",
    "    print(\"Score done. Save the best result.\")\n",
    "    bestResult.save(os.path.join(outDir, \"hyp.txt\") )\n",
    "    with open(os.path.join(outDir,\"best_PER\"),\"w\") as fw:\n",
    "        fw.write( f\"PER {bestWER[0]}, penalty {bestWER[1]}, LMWT {bestWER[2]}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train monophone GMM-HMM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "delta = 2 #(n-order to add to the feature)\n",
    "numIters = 40 # Training iterations\n",
    "maxIterInc = 30 # Final iteration of increasing gaussians\n",
    "realignIter = [1,2,3,4,5,6,7,8,9,10,12,14,16,18,20,23,26,29,32,35,38] # iteration to allign feature\n",
    "order = 5 # N-GRAM to use (min 1 | max 6)\n",
    "beam = 13 # Decode be                                          am size\n",
    "latBeam = 6 # Lattice beam size\n",
    "acwt = 0.083333 # Acoustic model weight\n",
    "parallel = 1 # parallel process to compute feature of train dataset (min 1 | max 10)\n",
    "skipTrain = False # If True, skip training. Do decoding only\n",
    "splice = 3 # How many left-right frames to splice\n",
    "mlltIter = [2,4,6,12] # The iteration estimate mllt matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load MFCC+CMVN feature.\n",
      "Add 2-order deltas.\n",
      "Restorage lexicon bank.\n"
     ]
    }
   ],
   "source": [
    "# ------------- Prepare feature for training ----------------------\n",
    "# 1. Load the feature for training (We use the index table format)\n",
    "feat = exkaldi.load_index_table(os.path.join(path_dataset,\"mfcc\",\"train\",\"mfcc_cmvn.ark\"))\n",
    "print(f\"Load MFCC+CMVN feature.\")\n",
    "feat = exkaldi.add_delta(feat, order=delta, outFile=os.path.join(path_dataset,\"train_mono\",\"mfcc_cmvn_delta.ark\"))\n",
    "print(f\"Add {delta}-order deltas.\")\n",
    "# 2. Load lexicon bank\n",
    "lexicons = exkaldi.load_lex(os.path.join(path_dataset,\"dict\",\"lexicons.lex\"))\n",
    "print(f\"Restorage lexicon bank.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized a monophone HMM-GMM model: GmmHmmInfo(phones=26, pdfs=82, transitionIds=180, transitionStates=82, dimension=39, gaussians=82).\n",
      "\n",
      "---------------------- Training -------------------------------------------------\n",
      "Start to train monophone model.\n",
      "Start Time: 2021/05/21-23:49:42\n",
      "Convert transcription to int value format.\n",
      "Compiling training graph.\n",
      "Iter >> 0\n",
      "Aligning data equally\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 377.6980 seconds\n",
      "Iter >> 1\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 3928.3835 seconds\n",
      "Iter >> 2\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 970.2897 seconds\n",
      "Iter >> 3\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 903.3061 seconds\n",
      "Iter >> 4\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 884.5455 seconds\n",
      "Iter >> 5\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 888.8432 seconds\n",
      "Iter >> 6\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 890.9345 seconds\n",
      "Iter >> 7\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 899.1174 seconds\n",
      "Iter >> 8\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 912.2794 seconds\n",
      "Iter >> 9\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 915.6244 seconds\n",
      "Iter >> 10\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 912.7548 seconds\n",
      "Iter >> 11\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 171.5744 seconds\n",
      "Iter >> 12\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 930.8017 seconds\n",
      "Iter >> 13\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 174.4369 seconds\n",
      "Iter >> 14\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 944.1080 seconds\n",
      "Iter >> 15\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 177.9625 seconds\n",
      "Iter >> 16\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 967.9956 seconds\n",
      "Iter >> 17\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 178.3776 seconds\n",
      "Iter >> 18\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 999.6342 seconds\n",
      "Iter >> 19\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 178.2974 seconds\n",
      "Iter >> 20\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 1020.8822 seconds\n",
      "Iter >> 21\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 184.3067 seconds\n",
      "Iter >> 22\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 182.7525 seconds\n",
      "Iter >> 23\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 1044.6408 seconds\n",
      "Iter >> 24\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 184.7487 seconds\n",
      "Iter >> 25\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 185.8413 seconds\n",
      "Iter >> 26\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 1065.9006 seconds\n",
      "Iter >> 27\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 188.7277 seconds\n",
      "Iter >> 28\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 187.8772 seconds\n",
      "Iter >> 29\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 1091.5555 seconds\n",
      "Iter >> 30\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 191.6440 seconds\n",
      "Iter >> 31\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 192.1833 seconds\n",
      "Iter >> 32\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 1108.5282 seconds\n",
      "Iter >> 33\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 193.8912 seconds\n",
      "Iter >> 34\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 196.5917 seconds\n",
      "Iter >> 35\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 1135.3192 seconds\n",
      "Iter >> 36\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 196.8192 seconds\n",
      "Iter >> 37\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 198.4140 seconds\n",
      "Iter >> 38\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 1154.3428 seconds\n",
      "Iter >> 39\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 197.7966 seconds\n",
      "Iter >> 40\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 201.9979 seconds\n",
      "Align last time with final model.\n",
      "Done to train the monophone model.\n",
      "Saved Final Model: /home/marrakchi/TFM/TFM_env/Datasets/ExKaldiASRDatasetFormat/train_mono/final.mdl\n",
      "Saved Alis:  /home/marrakchi/TFM/TFM_env/Datasets/ExKaldiASRDatasetFormat/train_mono/final.ali\n",
      "Saved tree: /home/marrakchi/TFM/TFM_env/Datasets/ExKaldiASRDatasetFormat/train_mono/tree\n",
      "End Time: 2021/05/22-07:47:27\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "GmmHmmInfo(phones=26, pdfs=82, transitionIds=180, transitionStates=82, dimension=39, gaussians=1252)\n",
      "Tree has been saved.\n",
      "Realign the training feature (boost silence = 1.25)\n",
      "Save the new alignment done.\n"
     ]
    }
   ],
   "source": [
    "# ------------- Start training ----------------------\n",
    "# 1. Initialize a monophone HMM object\n",
    "model = exkaldi.hmm.MonophoneHMM(lexicons=lexicons, name=\"mono\")\n",
    "model.initialize(\n",
    "            feat=feat,\n",
    "            topoFile=os.path.join(path_dataset,\"dict\",\"topo\")\n",
    "        )\n",
    "print(f\"Initialized a monophone HMM-GMM model: {model.info}.\")\n",
    "\n",
    "# 2. Split data for parallel training\n",
    "transcription = exkaldi.load_transcription(os.path.join(path_dataset,\"data\",\"train\",\"text\"))\n",
    "transcription = transcription.sort()\n",
    "if parallel > 1:\n",
    "    # split feature\n",
    "    feat = feat.sort(by=\"utt\").subset(chunks=parallel)\n",
    "    # split transcription depending on utterance IDs of each feature\n",
    "    temp = []\n",
    "    for f in feat:\n",
    "        temp.append( transcription.subset(keys=f.utts) )\n",
    "    transcription = temp\n",
    "    \n",
    "print(\"\\n---------------------- Training -------------------------------------------------\")\n",
    "# 3. Train\n",
    "model.train(\n",
    "            feat,\n",
    "            transcription, \n",
    "            LFile=os.path.join(path_dataset,\"dict\",\"L.fst\"),\n",
    "            tempDir=os.path.join(path_dataset,\"train_mono\"),\n",
    "            numIters=numIters, \n",
    "            maxIterInc=maxIterInc, \n",
    "            totgauss=1000, \n",
    "            realignIter=realignIter,\n",
    "            boostSilence=1.0,\n",
    "        )\n",
    "print(\"---------------------------------------------------------------------------------\\n\")\n",
    "print(model.info)\n",
    "# Save the tree\n",
    "model.tree.save(os.path.join(path_dataset,\"train_mono\",\"tree\"))\n",
    "print(f\"Tree has been saved.\")\n",
    "\n",
    "# 4. Realign with boostSilence 1.25\n",
    "print(\"Realign the training feature (boost silence = 1.25)\")\n",
    "trainGraphFiles = exkaldi.utils.list_files(os.path.join(path_dataset,\"train_mono\",\"*train_graph\"))\n",
    "model.align(\n",
    "            feat,\n",
    "            trainGraphFile=trainGraphFiles,  # train graphs have been generated in the train step.\n",
    "            boostSilence=1.25, #1.5\n",
    "            outFile=os.path.join(path_dataset,\"train_mono\",\"final.ali\")\n",
    "        )\n",
    "del feat\n",
    "print(\"Save the new alignment done.\")\n",
    "tree = model.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to make WFST graph.\n",
      "Load lexicon bank.\n",
      "Generate CLG fst done.\n",
      "Compose HCLG fst done.\n",
      "Load test feature.\n",
      "Feature type is delta. Add 2-order deltas.\n",
      "Start to decode\n",
      "Decode time cost:  4503.58 seconds\n",
      "Generate lattice done.\n",
      "Now score:\n",
      "Penalty: 0.0, LMWT: 1, PER: 30.31%\n",
      "Penalty: 0.0, LMWT: 2, PER: 29.96%\n",
      "Penalty: 0.0, LMWT: 3, PER: 29.64%\n",
      "Penalty: 0.0, LMWT: 4, PER: 29.46%\n",
      "Penalty: 0.0, LMWT: 5, PER: 29.31%\n",
      "Penalty: 0.0, LMWT: 6, PER: 29.32%\n",
      "Penalty: 0.0, LMWT: 7, PER: 29.49%\n",
      "Penalty: 0.0, LMWT: 8, PER: 29.77%\n",
      "Penalty: 0.0, LMWT: 9, PER: 30.19%\n",
      "Penalty: 0.0, LMWT: 10, PER: 30.64%\n",
      "Penalty: 0.5, LMWT: 1, PER: 30.33%\n",
      "Penalty: 0.5, LMWT: 2, PER: 29.95%\n",
      "Penalty: 0.5, LMWT: 3, PER: 29.68%\n",
      "Penalty: 0.5, LMWT: 4, PER: 29.6%\n",
      "Penalty: 0.5, LMWT: 5, PER: 29.48%\n",
      "Penalty: 0.5, LMWT: 6, PER: 29.51%\n",
      "Penalty: 0.5, LMWT: 7, PER: 29.74%\n",
      "Penalty: 0.5, LMWT: 8, PER: 30.1%\n",
      "Penalty: 0.5, LMWT: 9, PER: 30.6%\n",
      "Penalty: 0.5, LMWT: 10, PER: 31.15%\n",
      "Penalty: 1.0, LMWT: 1, PER: 30.32%\n",
      "Penalty: 1.0, LMWT: 2, PER: 29.98%\n",
      "Penalty: 1.0, LMWT: 3, PER: 29.75%\n",
      "Penalty: 1.0, LMWT: 4, PER: 29.7%\n",
      "Penalty: 1.0, LMWT: 5, PER: 29.66%\n",
      "Penalty: 1.0, LMWT: 6, PER: 29.8%\n",
      "Penalty: 1.0, LMWT: 7, PER: 30.17%\n",
      "Penalty: 1.0, LMWT: 8, PER: 30.6%\n",
      "Penalty: 1.0, LMWT: 9, PER: 31.18%\n",
      "Penalty: 1.0, LMWT: 10, PER: 31.85%\n",
      "Score done. Saved the best result.\n"
     ]
    }
   ],
   "source": [
    "# ------------- Compile WFST training ----------------------\n",
    "# Make a WFST decoding graph\n",
    "make_WFST_graph(\n",
    "            outDir=os.path.join(path_dataset,\"train_mono\",\"graph\"),\n",
    "            hmm=model,\n",
    "            tree=tree,\n",
    "        )\n",
    "\n",
    "# Decode test data\n",
    "GMM_decode_mfcc_and_score(\n",
    "            outDir=os.path.join(path_dataset,\"train_mono\",f\"decode_{order}grams\"), \n",
    "            hmm=model,\n",
    "            HCLGfile=os.path.join(path_dataset,\"train_mono\",\"graph\",f\"HCLG.{order}.fst\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now score:\n",
      "Penalty: 0.0, LMWT: 1, PER: 30.3%\n",
      "Penalty: 0.0, LMWT: 2, PER: 29.94%\n",
      "Penalty: 0.0, LMWT: 3, PER: 29.61%\n",
      "Penalty: 0.0, LMWT: 4, PER: 29.41%\n",
      "Penalty: 0.0, LMWT: 5, PER: 29.31%\n",
      "Penalty: 0.0, LMWT: 6, PER: 29.36%\n",
      "Penalty: 0.0, LMWT: 7, PER: 29.46%\n",
      "Penalty: 0.0, LMWT: 8, PER: 29.72%\n",
      "Penalty: 0.0, LMWT: 9, PER: 30.11%\n",
      "Penalty: 0.0, LMWT: 10, PER: 30.62%\n",
      "Penalty: 0.5, LMWT: 1, PER: 30.32%\n",
      "Penalty: 0.5, LMWT: 2, PER: 29.98%\n",
      "Penalty: 0.5, LMWT: 3, PER: 29.67%\n",
      "Penalty: 0.5, LMWT: 4, PER: 29.48%\n",
      "Penalty: 0.5, LMWT: 5, PER: 29.46%\n",
      "Penalty: 0.5, LMWT: 6, PER: 29.53%\n",
      "Penalty: 0.5, LMWT: 7, PER: 29.65%\n",
      "Penalty: 0.5, LMWT: 8, PER: 30.08%\n",
      "Penalty: 0.5, LMWT: 9, PER: 30.53%\n",
      "Penalty: 0.5, LMWT: 10, PER: 31.1%\n",
      "Penalty: 1.0, LMWT: 1, PER: 30.33%\n",
      "Penalty: 1.0, LMWT: 2, PER: 30.0%\n",
      "Penalty: 1.0, LMWT: 3, PER: 29.76%\n",
      "Penalty: 1.0, LMWT: 4, PER: 29.57%\n",
      "Penalty: 1.0, LMWT: 5, PER: 29.59%\n",
      "Penalty: 1.0, LMWT: 6, PER: 29.74%\n",
      "Penalty: 1.0, LMWT: 7, PER: 29.97%\n",
      "Penalty: 1.0, LMWT: 8, PER: 30.52%\n",
      "Penalty: 1.0, LMWT: 9, PER: 31.01%\n",
      "Penalty: 1.0, LMWT: 10, PER: 31.66%\n",
      "Score done. Saved the best result.\n"
     ]
    }
   ],
   "source": [
    "# Test score Without CMVN (only raw mfcc)\n",
    "\"\"\"outDir = os.path.join(path_dataset,\"train_mono\",f\"decode_{order}grams\")\n",
    "HCLGfile=os.path.join(path_dataset,\"train_mono\",\"graph\",f\"HCLG.{order}.fst\")\n",
    "\n",
    "# Load lattice file\n",
    "lat = exkaldi.load_lat(os.path.join(path_dataset,\"train_mono\",f\"decode_{order}grams\",\"test.lat\"))\n",
    "hmm = exkaldi.load_hmm(os.path.join(path_dataset,\"train_mono\",\"final.mdl\"))\n",
    "refText = exkaldi.load_transcription(os.path.join(path_dataset,\"data\",\"test\",\"text\"))\n",
    "\n",
    "# Transform transcriptions from words to phones\n",
    "word2phone = exkaldi.load_list_table(path_dataset+'word2phone.map')\n",
    "refText = refText.convert(word2phone, unkSymbol='<UNK>')\n",
    "\n",
    "print(\"Now score:\")\n",
    "bestWER = (1000, 0, 0)\n",
    "bestResult = None\n",
    "for penalty in [0., 0.5, 1.0]:\n",
    "    for LMWT in range(1, 11):\n",
    "        # Add penalty\n",
    "        newLat = lat.add_penalty(penalty)\n",
    "        # Get 1-best result (phone-level)\n",
    "        result = newLat.get_1best(lexicons(\"phones\"), hmm, lmwt=LMWT, acwt=1, phoneLevel=True)\n",
    "        # Transform from int value format to text format\n",
    "        result = exkaldi.hmm.transcription_from_int(result, lexicons(\"phones\"))\n",
    "        # Compute PER\n",
    "        score = exkaldi.decode.score.wer(ref=refText, hyp=result, mode=\"present\")\n",
    "        if score.WER < bestWER[0]:\n",
    "            bestResult = result\n",
    "            bestWER = (score.WER, penalty, LMWT)\n",
    "        print(f\"Penalty: {penalty}, LMWT: {LMWT}, PER: {score.WER}%\")\n",
    "\n",
    "print(\"Score done. Saved the best result.\")\n",
    "bestResult.save(os.path.join(outDir, \"hyp.txt\") )\n",
    "with open(os.path.join(outDir,\"best_PER\"),\"w\") as fw:\n",
    "    fw.write( f\"PER {bestWER[0]}, penalty {bestWER[1]}, LMWT {bestWER[2]}\" )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train triphone GMM-HMM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "delta = 2 #(n-order to add to the feature)\n",
    "numIters = 35 # Training iterations\n",
    "maxIterInc = 25 # Final iteration of increasing gaussians\n",
    "realignIter = [10,20,30] # iteration to allign feature\n",
    "order = 5 # N-GRAM to use (min 1 | max 6)\n",
    "beam = 13 # Decode be                                          am size\n",
    "latBeam = 6 # Lattice beam size\n",
    "acwt = 0.083333 # Acoustic model weight\n",
    "parallel = 1 # parallel process to compute feature of train dataset (min 1 | max 10)\n",
    "skipTrain = False # If True, skip training. Do decoding only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load MFCC+CMVN feature.\n",
      "Add 2-order deltas.\n",
      "Restorage lexicon bank.\n"
     ]
    }
   ],
   "source": [
    "# ------------- Prepare feature and previous alignment for training ----------------------\n",
    "# 1. Load the feature for training\n",
    "feat = exkaldi.load_index_table(os.path.join(path_dataset,\"mfcc\",\"train\",\"mfcc_cmvn.ark\"))\n",
    "print(f\"Load MFCC+CMVN feature.\")\n",
    "feat = exkaldi.add_delta(feat, order=delta, outFile=os.path.join(path_dataset,\"train_delta\",\"mfcc_cmvn_delta.ark\"))\n",
    "print(f\"Add {delta}-order deltas.\")\n",
    "# 2. Load lexicon bank\n",
    "lexicons = exkaldi.load_lex(os.path.join(path_dataset,\"dict\",\"lexicons.lex\"))\n",
    "print(f\"Restorage lexicon bank.\")\n",
    "# 3. Load previous alignment\n",
    "ali = exkaldi.load_index_table(os.path.join(path_dataset,\"train_mono\",\"*final.ali\"),useSuffix=\"ark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start build a tree\n",
      "Start to build decision tree.\n",
      "Start Time: 2021/05/23-00:06:37\n",
      "Accumulate tree statistics\n",
      "Cluster phones and compile questions\n",
      "Build tree\n",
      "Done to build the decision tree.\n",
      "Saved Final Tree: /home/marrakchi/TFM/TFM_env/Datasets/ExKaldiASRDatasetFormat/train_delta/tree\n",
      "End Time: 2021/05/23-00:09:35\n",
      "Build tree done.\n"
     ]
    }
   ],
   "source": [
    "# -------------- Build the decision tree ------------------------\n",
    "print(\"Start build a tree\")\n",
    "tree = exkaldi.hmm.DecisionTree(lexicons=lexicons, contextWidth=3, centralPosition=1)\n",
    "tree.train(\n",
    "            feat=feat, \n",
    "            hmm=os.path.join(path_dataset,\"train_mono\",\"final.mdl\"), \n",
    "            ali=ali, \n",
    "            topoFile=os.path.join(path_dataset,\"dict\",\"topo\"), \n",
    "            numLeaves=2500,\n",
    "            tempDir=os.path.join(path_dataset,\"train_delta\"), \n",
    "        )\n",
    "print(f\"Build tree done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized a triphone HMM-GMM model: GmmHmmInfo(phones=26, pdfs=2088, transitionIds=4258, transitionStates=2121, dimension=39, gaussians=2088).\n",
      "Transform the alignment\n"
     ]
    }
   ],
   "source": [
    "# ------------- Start training ----------------------\n",
    "# 1. Initialize a triphone HMM object\n",
    "model = exkaldi.hmm.TriphoneHMM(lexicons=lexicons, name=\"tri\")\n",
    "model.initialize(\n",
    "            tree=tree, \n",
    "            topoFile=os.path.join(path_dataset,\"dict\",\"topo\"),\n",
    "            treeStatsFile=os.path.join(path_dataset,\"train_delta\",\"treeStats.acc\"),\n",
    "        )\n",
    "print(f\"Initialized a triphone HMM-GMM model: {model.info}.\")\n",
    "\n",
    "# 2. convert the previous alignment\n",
    "print(f\"Transform the alignment\")\n",
    "newAli = exkaldi.hmm.convert_alignment(\n",
    "                                ali=ali,\n",
    "                                originHmm=os.path.join(path_dataset,\"train_mono\",\"final.mdl\"), \n",
    "                                targetHmm=model, \n",
    "                                tree=tree,\n",
    "                                outFile=os.path.join(path_dataset,\"train_delta\",\"initial.ali\"),\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Train the triphone model--------------------------------------\n",
      "Start to train triphone model.\n",
      "Start Time: 2021/05/23-00:12:47\n",
      "Convert transcription to int value format.\n",
      "Compiling training graph.\n",
      "Iter >> 1\n",
      "Use the provided initial alignment\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 164.2384 seconds\n",
      "Iter >> 2\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 161.2999 seconds\n",
      "Iter >> 3\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 171.5843 seconds\n",
      "Iter >> 4\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 168.7223 seconds\n",
      "Iter >> 5\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 162.1427 seconds\n",
      "Iter >> 6\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 169.2235 seconds\n",
      "Iter >> 7\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 167.6211 seconds\n",
      "Iter >> 8\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 168.3272 seconds\n",
      "Iter >> 9\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 171.3398 seconds\n",
      "Iter >> 10\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 976.9799 seconds\n",
      "Iter >> 11\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 175.4487 seconds\n",
      "Iter >> 12\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 175.5024 seconds\n",
      "Iter >> 13\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 173.9635 seconds\n",
      "Iter >> 14\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 177.3075 seconds\n",
      "Iter >> 15\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 175.7446 seconds\n",
      "Iter >> 16\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 178.7644 seconds\n",
      "Iter >> 17\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 178.5840 seconds\n",
      "Iter >> 18\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 179.2961 seconds\n",
      "Iter >> 19\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 182.2098 seconds\n",
      "Iter >> 20\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 1066.4881 seconds\n",
      "Iter >> 21\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 184.9546 seconds\n",
      "Iter >> 22\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 192.1093 seconds\n",
      "Iter >> 23\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 186.3232 seconds\n",
      "Iter >> 24\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 189.4308 seconds\n",
      "Iter >> 25\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 187.3320 seconds\n",
      "Iter >> 26\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 191.9688 seconds\n",
      "Iter >> 27\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 185.6806 seconds\n",
      "Iter >> 28\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 191.6766 seconds\n",
      "Iter >> 29\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 191.7966 seconds\n",
      "Iter >> 30\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 1087.8108 seconds\n",
      "Iter >> 31\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 197.6323 seconds\n",
      "Iter >> 32\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 195.7169 seconds\n",
      "Iter >> 33\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 195.8220 seconds\n",
      "Iter >> 34\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 202.2368 seconds\n",
      "Iter >> 35\n",
      "Skip aligning\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 201.7949 seconds\n",
      "Align last time with final model\n",
      "Done to train the triphone model\n",
      "Saved Final Model: /home/marrakchi/TFM/TFM_env/Datasets/ExKaldiASRDatasetFormat/train_delta/final.mdl\n",
      "Saved Ali:  /home/marrakchi/TFM/TFM_env/Datasets/ExKaldiASRDatasetFormat/train_delta/final.ali\n",
      "End Time: 2021/05/23-03:01:14\n",
      "GmmHmmInfo(phones=26, pdfs=2088, transitionIds=4258, transitionStates=2121, dimension=39, gaussians=19632)\n",
      "Tree has been saved.\n"
     ]
    }
   ],
   "source": [
    "# 2. Split data for parallel training\n",
    "transcription = exkaldi.load_transcription(os.path.join(path_dataset,\"data\",\"train\",\"text\"))\n",
    "transcription = transcription.sort()\n",
    "if parallel > 1:\n",
    "    # split feature\n",
    "    feat = feat.sort(by=\"utt\").subset(chunks=parallel)\n",
    "    # split transcription depending on utterance IDs of each feat\n",
    "    tempTrans = []\n",
    "    tempAli = []\n",
    "    for f in feat:\n",
    "        tempTrans.append( transcription.subset(keys=f.utts) )\n",
    "        tempAli.append( newAli.subset(keys=f.utts) )\n",
    "    transcription = tempTrans\n",
    "    newAli = tempAli\n",
    "\n",
    "# 3. Train\n",
    "print(\"----Train the triphone model--------------------------------------\")\n",
    "model.train(feat,\n",
    "            transcription, \n",
    "            os.path.join(path_dataset,\"dict\",\"L.fst\"), \n",
    "            tree,\n",
    "            tempDir=os.path.join(path_dataset,\"train_delta\"),\n",
    "            initialAli=newAli,\n",
    "            numIters=numIters, \n",
    "            maxIterInc=maxIterInc,\n",
    "            totgauss=15000,\n",
    "            realignIter=realignIter,\n",
    "            boostSilence=1.0,\n",
    "        )\n",
    "print(model.info)\n",
    "# Save the tree\n",
    "model.tree.save(os.path.join(path_dataset,\"train_delta\",\"tree\"))\n",
    "print(f\"Tree has been saved.\")\n",
    "del feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to make WFST graph.\n",
      "Load lexicon bank.\n",
      "Generate CLG fst done.\n",
      "Compose HCLG fst done.\n",
      "Load test feature.\n",
      "Feature type is delta. Add 2-order deltas.\n",
      "Start to decode\n",
      "Decode time cost:  1490.50 seconds\n",
      "Generate lattice done.\n",
      "Now score:\n",
      "Penalty: 0.0, LMWT: 1, PER: 25.16%\n",
      "Penalty: 0.0, LMWT: 2, PER: 24.95%\n",
      "Penalty: 0.0, LMWT: 3, PER: 24.69%\n",
      "Penalty: 0.0, LMWT: 4, PER: 24.48%\n",
      "Penalty: 0.0, LMWT: 5, PER: 24.27%\n",
      "Penalty: 0.0, LMWT: 6, PER: 24.04%\n",
      "Penalty: 0.0, LMWT: 7, PER: 23.83%\n",
      "Penalty: 0.0, LMWT: 8, PER: 23.72%\n",
      "Penalty: 0.0, LMWT: 9, PER: 23.61%\n",
      "Penalty: 0.0, LMWT: 10, PER: 23.62%\n",
      "Penalty: 0.5, LMWT: 1, PER: 25.15%\n",
      "Penalty: 0.5, LMWT: 2, PER: 24.93%\n",
      "Penalty: 0.5, LMWT: 3, PER: 24.66%\n",
      "Penalty: 0.5, LMWT: 4, PER: 24.44%\n",
      "Penalty: 0.5, LMWT: 5, PER: 24.21%\n",
      "Penalty: 0.5, LMWT: 6, PER: 23.99%\n",
      "Penalty: 0.5, LMWT: 7, PER: 23.85%\n",
      "Penalty: 0.5, LMWT: 8, PER: 23.68%\n",
      "Penalty: 0.5, LMWT: 9, PER: 23.68%\n",
      "Penalty: 0.5, LMWT: 10, PER: 23.69%\n",
      "Penalty: 1.0, LMWT: 1, PER: 25.13%\n",
      "Penalty: 1.0, LMWT: 2, PER: 24.89%\n",
      "Penalty: 1.0, LMWT: 3, PER: 24.64%\n",
      "Penalty: 1.0, LMWT: 4, PER: 24.41%\n",
      "Penalty: 1.0, LMWT: 5, PER: 24.18%\n",
      "Penalty: 1.0, LMWT: 6, PER: 23.96%\n",
      "Penalty: 1.0, LMWT: 7, PER: 23.81%\n",
      "Penalty: 1.0, LMWT: 8, PER: 23.67%\n",
      "Penalty: 1.0, LMWT: 9, PER: 23.68%\n",
      "Penalty: 1.0, LMWT: 10, PER: 23.73%\n",
      "Score done. Saved the best result.\n"
     ]
    }
   ],
   "source": [
    "#declare.is_file( os.path.join(path_dataset,\"train_delta\",\"final.mdl\") )\n",
    "#declare.is_file( os.path.join(path_dataset,\"train_delta\",\"tree\") )\n",
    "    \n",
    "#model = exkaldi.load_hmm(os.path.join(path_dataset,\"train_delta\",\"final.mdl\"))\n",
    "#tree = exkaldi.load_tree(os.path.join(path_dataset,\"train_delta\",\"tree\"))\n",
    "\n",
    "# Results with CMVN and less realignIter\n",
    "# ------------- Compile WFST training ---------------------- \n",
    "# Make a WFST decoding graph\n",
    "make_WFST_graph(\n",
    "            outDir=os.path.join(path_dataset,\"train_delta\",\"graph\"),\n",
    "            hmm=model,\n",
    "            tree=tree,\n",
    "        )\n",
    "# Decode test data\n",
    "GMM_decode_mfcc_and_score(\n",
    "            outDir=os.path.join(path_dataset,\"train_delta\",f\"decode_{order}grams\"), \n",
    "            hmm=model,\n",
    "            HCLGfile=os.path.join(path_dataset,\"train_delta\",\"graph\",f\"HCLG.{order}.fst\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to make WFST graph.\n",
      "Load lexicon bank.\n",
      "Generate CLG fst done.\n",
      "Compose HCLG fst done.\n",
      "Load test feature.\n",
      "Feature type is delta. Add 2-order deltas.\n",
      "Start to decode\n",
      "Decode time cost:  921.69 seconds\n",
      "Generate lattice done.\n",
      "Now score:\n",
      "Penalty: 0.0, LMWT: 1, PER: 25.08%\n",
      "Penalty: 0.0, LMWT: 2, PER: 24.85%\n",
      "Penalty: 0.0, LMWT: 3, PER: 24.58%\n",
      "Penalty: 0.0, LMWT: 4, PER: 24.3%\n",
      "Penalty: 0.0, LMWT: 5, PER: 23.98%\n",
      "Penalty: 0.0, LMWT: 6, PER: 23.79%\n",
      "Penalty: 0.0, LMWT: 7, PER: 23.62%\n",
      "Penalty: 0.0, LMWT: 8, PER: 23.5%\n",
      "Penalty: 0.0, LMWT: 9, PER: 23.42%\n",
      "Penalty: 0.0, LMWT: 10, PER: 23.39%\n",
      "Penalty: 0.5, LMWT: 1, PER: 25.09%\n",
      "Penalty: 0.5, LMWT: 2, PER: 24.82%\n",
      "Penalty: 0.5, LMWT: 3, PER: 24.54%\n",
      "Penalty: 0.5, LMWT: 4, PER: 24.22%\n",
      "Penalty: 0.5, LMWT: 5, PER: 23.94%\n",
      "Penalty: 0.5, LMWT: 6, PER: 23.76%\n",
      "Penalty: 0.5, LMWT: 7, PER: 23.59%\n",
      "Penalty: 0.5, LMWT: 8, PER: 23.5%\n",
      "Penalty: 0.5, LMWT: 9, PER: 23.43%\n",
      "Penalty: 0.5, LMWT: 10, PER: 23.41%\n",
      "Penalty: 1.0, LMWT: 1, PER: 25.08%\n",
      "Penalty: 1.0, LMWT: 2, PER: 24.8%\n",
      "Penalty: 1.0, LMWT: 3, PER: 24.49%\n",
      "Penalty: 1.0, LMWT: 4, PER: 24.19%\n",
      "Penalty: 1.0, LMWT: 5, PER: 23.93%\n",
      "Penalty: 1.0, LMWT: 6, PER: 23.73%\n",
      "Penalty: 1.0, LMWT: 7, PER: 23.62%\n",
      "Penalty: 1.0, LMWT: 8, PER: 23.49%\n",
      "Penalty: 1.0, LMWT: 9, PER: 23.46%\n",
      "Penalty: 1.0, LMWT: 10, PER: 23.48%\n",
      "Score done. Saved the best result.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Resutls without CMVN (with only raw mfcc)\n",
    "# ------------- Compile WFST training ---------------------- \n",
    "# Make a WFST decoding graph\n",
    "make_WFST_graph(\n",
    "            outDir=os.path.join(path_dataset,\"train_delta\",\"graph\"),\n",
    "            hmm=model,\n",
    "            tree=tree,\n",
    "        )\n",
    "# Decode test data\n",
    "GMM_decode_mfcc_and_score(\n",
    "            outDir=os.path.join(path_dataset,\"train_delta\",f\"decode_{order}grams\"), \n",
    "            hmm=model,\n",
    "            HCLGfile=os.path.join(path_dataset,\"train_delta\",\"graph\",f\"HCLG.{order}.fst\"),\n",
    "        )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GMM-HMM model with LDA+feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "delta = 2 #(n-order to add to the feature)\n",
    "numIters = 35 # Training iterations\n",
    "maxIterInc = 25 # Final iteration of increasing gaussians\n",
    "realignIter = [10,20,30] # iteration to allign feature\n",
    "order = 5 # N-GRAM to use (min 1 | max 6)\n",
    "beam = 13 # Decode beam size\n",
    "latBeam = 6 # Lattice beam size\n",
    "acwt = 0.083333 # Acoustic model weight\n",
    "parallel = 1 # parallel process to compute feature of train dataset (min 1 | max 10)\n",
    "skipTrain = False # If True, skip training. Do decoding only\n",
    "splice = 3 # How many left-right frames to splice\n",
    "mlltIter = [2,4,6,12] # The iteration estimate mllt matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load MFCC+CMVN feature.\n",
      "Splice 3 frames.\n",
      "Estimate LDA matrix.\n",
      "Transform feature\n"
     ]
    }
   ],
   "source": [
    "# ------------- Prepare feature and previous alignment for training ----------------------\n",
    "# 1. Load the feature for training\n",
    "print(f\"Load MFCC+CMVN feature.\")\n",
    "feat = exkaldi.load_index_table(os.path.join(path_dataset,\"mfcc\",\"train\",\"mfcc_cmvn.ark\"))\n",
    "print(f\"Splice {splice} frames.\")\n",
    "originalFeat = exkaldi.splice_feature(feat,left=splice,right=splice,outFile=os.path.join(path_dataset,\"train_delta\",\"mfcc_cmvn_splice.ark\"))\n",
    "# 2. Load previous alignment and lexicons\n",
    "ali = exkaldi.load_index_table(os.path.join(path_dataset,\"train_delta\",\"*final.ali\"),useSuffix=\"ark\")\n",
    "lexicons = exkaldi.load_lex(os.path.join(path_dataset,\"dict\",\"lexicons.lex\"))\n",
    "# 3. Estimate LDA matrix and convert feature\n",
    "print(\"Estimate LDA matrix.\")\n",
    "exkaldi.hmm.accumulate_LDA_stats(\n",
    "                ali=ali, \n",
    "                lexicons=lexicons, \n",
    "                hmm=os.path.join(path_dataset,\"train_delta\",\"final.mdl\"), \n",
    "                feat=originalFeat, \n",
    "                outFile=os.path.join(path_dataset,\"train_lda_mllt\",\"ldaStats.acc\"),\n",
    "                silenceWeight=0,\n",
    "                randPrune=4,\n",
    "            )\n",
    "exkaldi.hmm.estimate_LDA_matrix(\n",
    "                            statsFiles=os.path.join(path_dataset,\"train_lda_mllt\",\"*ldaStats.acc\"), \n",
    "                            targetDim=40, \n",
    "                            outFile=os.path.join(path_dataset,\"train_lda_mllt\",\"trans.mat\")\n",
    "                        )\n",
    "print(\"Transform feature\")\n",
    "ldaFeat = exkaldi.transform_feat(\n",
    "                        feat=originalFeat, \n",
    "                        matFile=os.path.join(path_dataset,\"train_lda_mllt\",\"trans.mat\"),\n",
    "                        outFile=os.path.join(path_dataset,\"train_lda_mllt\",\"lda_feat.ark\"),\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start build a tree\n",
      "Start to build decision tree.\n",
      "Start Time: 2021/05/22-15:21:02\n",
      "Accumulate tree statistics\n",
      "Cluster phones and compile questions\n",
      "Build tree\n",
      "Done to build the decision tree.\n",
      "Saved Final Tree: /home/marrakchi/TFM/TFM_env/Datasets/ExKaldiASRDatasetFormat/train_lda_mllt/tree\n",
      "End Time: 2021/05/22-15:24:22\n",
      "Build tree done.\n",
      "Initialize a triphone HMM object\n",
      "Initialized a monophone HMM-GMM model: GmmHmmInfo(phones=26, pdfs=2160, transitionIds=4422, transitionStates=2203, dimension=40, gaussians=2160).\n",
      "Transform the alignment\n"
     ]
    }
   ],
   "source": [
    "# -------------- Build the decision tree ------------------------\n",
    "print(\"Start build a tree\")\n",
    "tree = exkaldi.hmm.DecisionTree(lexicons=lexicons, contextWidth=3, centralPosition=1)\n",
    "tree.train(\n",
    "            feat=ldaFeat, \n",
    "            hmm=os.path.join(path_dataset,\"train_delta\",\"final.mdl\"), \n",
    "            ali=ali,\n",
    "            topoFile=os.path.join(path_dataset,\"dict\",\"topo\"), \n",
    "            numLeaves=2500,\n",
    "            tempDir=os.path.join(path_dataset,\"train_lda_mllt\"), \n",
    "        )\n",
    "tree.save(os.path.join(path_dataset,\"train_delta\",\"tree\"))\n",
    "print(f\"Build tree done.\")\n",
    "del ldaFeat\n",
    "\n",
    "# ------------- Start training ----------------------\n",
    "# 1. Initialize a triphone HMM object\n",
    "print(\"Initialize a triphone HMM object\")\n",
    "model = exkaldi.hmm.TriphoneHMM(lexicons=lexicons)\n",
    "model.initialize(\n",
    "            tree=tree, \n",
    "            topoFile=os.path.join(path_dataset,\"dict\",\"topo\"),\n",
    "            treeStatsFile=os.path.join(path_dataset,\"train_lda_mllt\",\"treeStats.acc\"), \n",
    "        )    \n",
    "print(f\"Initialized a monophone HMM-GMM model: {model.info}.\")\n",
    "\n",
    "# 2. convert the previous alignment\n",
    "print(f\"Transform the alignment\")\n",
    "newAli = exkaldi.hmm.convert_alignment(\n",
    "                                ali=ali,\n",
    "                                originHmm=os.path.join(path_dataset,\"train_delta\",\"final.mdl\"), \n",
    "                                targetHmm=model, \n",
    "                                tree=tree,\n",
    "                                outFile=os.path.join(path_dataset,\"train_lda_mllt\",\"initial.ali\"),\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the triphone model\n",
      "Do LDA + MLLT training.\n",
      "Start to train triphone model.\n",
      "Start Time: 2021/05/22-15:34:48\n",
      "Convert transcription to int value format.\n",
      "Compiling training graph.\n",
      "Iter >> 1\n",
      "Use the provided initial alignment\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 198.4770 seconds\n",
      "Iter >> 2\n",
      "Skip aligning\n",
      "Accumulate MLLT statistics\n",
      "Estimate MLLT matrix\n",
      "Transform GMM means\n",
      "Compose new LDA-MLLT transform matrix\n",
      "Transform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 877.7089 seconds\n",
      "Iter >> 3\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 181.1236 seconds\n",
      "Iter >> 4\n",
      "Skip aligning\n",
      "Accumulate MLLT statistics\n",
      "Estimate MLLT matrix\n",
      "Transform GMM means\n",
      "Compose new LDA-MLLT transform matrix\n",
      "Transform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 908.5012 seconds\n",
      "Iter >> 5\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 185.1446 seconds\n",
      "Iter >> 6\n",
      "Skip aligning\n",
      "Accumulate MLLT statistics\n",
      "Estimate MLLT matrix\n",
      "Transform GMM means\n",
      "Compose new LDA-MLLT transform matrix\n",
      "Transform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 876.0942 seconds\n",
      "Iter >> 7\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 191.0187 seconds\n",
      "Iter >> 8\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 187.1435 seconds\n",
      "Iter >> 9\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 190.8133 seconds\n",
      "Iter >> 10\n",
      "Aligning data\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 925.4685 seconds\n",
      "Iter >> 11\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 198.3383 seconds\n",
      "Iter >> 12\n",
      "Skip aligning\n",
      "Accumulate MLLT statistics\n",
      "Estimate MLLT matrix\n",
      "Transform GMM means\n",
      "Compose new LDA-MLLT transform matrix\n",
      "Transform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 875.8442 seconds\n",
      "Iter >> 13\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 189.7367 seconds\n",
      "Iter >> 14\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 192.9957 seconds\n",
      "Iter >> 15\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 186.3662 seconds\n",
      "Iter >> 16\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 184.6486 seconds\n",
      "Iter >> 17\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 189.4417 seconds\n",
      "Iter >> 18\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 193.9316 seconds\n",
      "Iter >> 19\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 188.5263 seconds\n",
      "Iter >> 20\n",
      "Aligning data\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 983.0189 seconds\n",
      "Iter >> 21\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 197.8871 seconds\n",
      "Iter >> 22\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 199.1304 seconds\n",
      "Iter >> 23\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 205.5432 seconds\n",
      "Iter >> 24\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 202.0163 seconds\n",
      "Iter >> 25\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 201.6541 seconds\n",
      "Iter >> 26\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 205.9747 seconds\n",
      "Iter >> 27\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 210.2826 seconds\n",
      "Iter >> 28\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 208.2159 seconds\n",
      "Iter >> 29\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 212.6480 seconds\n",
      "Iter >> 30\n",
      "Aligning data\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 1056.9725 seconds\n",
      "Iter >> 31\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 212.1115 seconds\n",
      "Iter >> 32\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 234.0456 seconds\n",
      "Iter >> 33\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 212.3414 seconds\n",
      "Iter >> 34\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 219.8790 seconds\n",
      "Iter >> 35\n",
      "Skip aligning\n",
      "Skip tansform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 223.8837 seconds\n",
      "Align last time with final model\n",
      "Done to train the triphone model\n",
      "Saved Final Model: /home/marrakchi/TFM/TFM_env/Datasets/ExKaldiASRDatasetFormat/train_lda_mllt/final.mdl\n",
      "Saved Ali:  /home/marrakchi/TFM/TFM_env/Datasets/ExKaldiASRDatasetFormat/train_lda_mllt/final.ali\n",
      "Saved Feat Transform Matrix: /home/marrakchi/TFM/TFM_env/Datasets/ExKaldiASRDatasetFormat/train_lda_mllt/trans.mat\n",
      "End Time: 2021/05/22-19:15:39\n",
      "GmmHmmInfo(phones=26, pdfs=2160, transitionIds=4422, transitionStates=2203, dimension=40, gaussians=19602)\n"
     ]
    }
   ],
   "source": [
    "# 2. Split data for parallel training\n",
    "transcription = exkaldi.load_transcription(os.path.join(path_dataset,\"data\",\"train\",\"text\"))\n",
    "transcription = transcription.sort()\n",
    "if parallel > 1:\n",
    "    # split feature\n",
    "    originalFeat = originalFeat.sort(by=\"utt\").subset(chunks=parallel)\n",
    "    # split transcription depending on utterance IDs of each feat\n",
    "    tempTrans = []\n",
    "    tempAli = []\n",
    "    for f in originalFeat:\n",
    "        tempTrans.append( transcription.subset(keys=f.utts) )\n",
    "        tempAli.append( newAli.subset(keys=f.utts) )\n",
    "    transcription = tempTrans\n",
    "    newAli = tempAli\n",
    "\n",
    "# 3. Train\n",
    "print(\"Train the triphone model\")\n",
    "model.train(\n",
    "            originalFeat,\n",
    "            transcription, \n",
    "            os.path.join(path_dataset,\"dict\",\"L.fst\"), \n",
    "            tree,\n",
    "            tempDir=os.path.join(path_dataset,\"train_lda_mllt\"),\n",
    "            initialAli=newAli,\n",
    "            ldaMatFile=os.path.join(path_dataset,\"train_lda_mllt\",\"trans.mat\"),\n",
    "            numIters=numIters, \n",
    "            maxIterInc=maxIterInc,\n",
    "            totgauss=15000,\n",
    "            realignIter=realignIter,\n",
    "            mlltIter=mlltIter,\n",
    "        )\n",
    "print(model.info)\n",
    "# Save the tree\n",
    "del originalFeat\n",
    "del newAli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to make WFST graph.\n",
      "Load lexicon bank.\n",
      "Generate CLG fst done.\n",
      "Compose HCLG fst done.\n",
      "Load test feature.\n",
      "Feature type is lda+mllt. Transform feature.\n",
      "Start to decode\n",
      "Decode time cost:  1271.29 seconds\n",
      "Generate lattice done.\n",
      "Now score:\n",
      "Penalty: 0.0, LMWT: 1, PER: 24.42%\n",
      "Penalty: 0.0, LMWT: 2, PER: 24.16%\n",
      "Penalty: 0.0, LMWT: 3, PER: 23.92%\n",
      "Penalty: 0.0, LMWT: 4, PER: 23.67%\n",
      "Penalty: 0.0, LMWT: 5, PER: 23.45%\n",
      "Penalty: 0.0, LMWT: 6, PER: 23.31%\n",
      "Penalty: 0.0, LMWT: 7, PER: 23.12%\n",
      "Penalty: 0.0, LMWT: 8, PER: 23.02%\n",
      "Penalty: 0.0, LMWT: 9, PER: 22.93%\n",
      "Penalty: 0.0, LMWT: 10, PER: 22.87%\n",
      "Penalty: 0.5, LMWT: 1, PER: 24.41%\n",
      "Penalty: 0.5, LMWT: 2, PER: 24.13%\n",
      "Penalty: 0.5, LMWT: 3, PER: 23.88%\n",
      "Penalty: 0.5, LMWT: 4, PER: 23.62%\n",
      "Penalty: 0.5, LMWT: 5, PER: 23.41%\n",
      "Penalty: 0.5, LMWT: 6, PER: 23.24%\n",
      "Penalty: 0.5, LMWT: 7, PER: 23.13%\n",
      "Penalty: 0.5, LMWT: 8, PER: 23.02%\n",
      "Penalty: 0.5, LMWT: 9, PER: 22.94%\n",
      "Penalty: 0.5, LMWT: 10, PER: 22.86%\n",
      "Penalty: 1.0, LMWT: 1, PER: 24.39%\n",
      "Penalty: 1.0, LMWT: 2, PER: 24.11%\n",
      "Penalty: 1.0, LMWT: 3, PER: 23.85%\n",
      "Penalty: 1.0, LMWT: 4, PER: 23.57%\n",
      "Penalty: 1.0, LMWT: 5, PER: 23.38%\n",
      "Penalty: 1.0, LMWT: 6, PER: 23.21%\n",
      "Penalty: 1.0, LMWT: 7, PER: 23.12%\n",
      "Penalty: 1.0, LMWT: 8, PER: 23.02%\n",
      "Penalty: 1.0, LMWT: 9, PER: 22.94%\n",
      "Penalty: 1.0, LMWT: 10, PER: 22.89%\n",
      "Score done. Saved the best result.\n"
     ]
    }
   ],
   "source": [
    "declare.is_file( os.path.join(path_dataset,\"train_lda_mllt\",\"final.mdl\") )\n",
    "declare.is_file( os.path.join(path_dataset,\"train_lda_mllt\",\"tree\") )\n",
    "    \n",
    "model = exkaldi.load_hmm(os.path.join(path_dataset,\"train_lda_mllt\",\"final.mdl\"))\n",
    "tree = exkaldi.load_tree(os.path.join(path_dataset,\"train_lda_mllt\",\"tree\"))\n",
    "        \n",
    "# ------------- Compile WFST training ----------------------\n",
    "# Make a WFST decoding graph\n",
    "make_WFST_graph(\n",
    "            outDir=os.path.join(path_dataset,\"train_lda_mllt\",\"graph\"),\n",
    "            hmm=model,\n",
    "            tree=tree,\n",
    "        )\n",
    "# Decode test data\n",
    "GMM_decode_mfcc_and_score(\n",
    "            outDir=os.path.join(path_dataset,\"train_lda_mllt\",f\"decode_{order}grams\"), \n",
    "            hmm=model,\n",
    "            HCLGfile=os.path.join(path_dataset,\"train_lda_mllt\",\"graph\",f\"HCLG.{order}.fst\"),\n",
    "            tansformMatFile=os.path.join(path_dataset,\"train_lda_mllt\",\"trans.mat\"),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a GMM-HMM model with fmllr feature.\n",
    "## --> Speaker Adapted Training (SAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "delta = 2 #(n-order to add to the feature)\n",
    "numIters = 35 # Training iterations\n",
    "maxIterInc = 25 # Final iteration of increasing gaussians\n",
    "realignIter = [10,20,30] # iteration to allign feature\n",
    "order = 5 # N-GRAM to use (min 1 | max 6)\n",
    "beam = 13 # Decode be                                          am size\n",
    "latBeam = 6 # Lattice beam size\n",
    "acwt = 0.083333 # Acoustic model weight\n",
    "parallel = 1 # parallel process to compute feature of train dataset (min 1 | max 10)\n",
    "skipTrain = False # If True, skip training. Do decoding only\n",
    "splice = 3 # How many left-right frames to splice\n",
    "fmllrIter = [2,4,6,12] # The iteration to estimate fmllr matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load MFCC+CMVN feature.\n",
      "Splice 3 frames.\n",
      "Transform LDA feature\n",
      "Estiminate the primary fMLLR transform matrixs\n"
     ]
    }
   ],
   "source": [
    "# ------------- Prepare feature and previous alignment for training ----------------------\n",
    "# 1. Load the feature for training\n",
    "print(f\"Load MFCC+CMVN feature.\")\n",
    "feat = exkaldi.load_index_table(os.path.join(path_dataset,\"mfcc\",\"train\",\"mfcc_cmvn.ark\"))\n",
    "print(f\"Splice {splice} frames.\")\n",
    "originalFeat = exkaldi.splice_feature(feat,left=splice,right=splice,outFile=os.path.join(path_dataset,\"train_delta\",\"mfcc_cmvn_splice.ark\"))\n",
    "print(f\"Transform LDA feature\")\n",
    "ldaFeat = exkaldi.transform_feat(\n",
    "                        feat=originalFeat, \n",
    "                        matFile=os.path.join(path_dataset,\"train_lda_mllt\",\"trans.mat\"),\n",
    "                        outFile=os.path.join(path_dataset,\"train_sat\",\"lda_feat.ark\"),\n",
    "                    )\n",
    "del originalFeat\n",
    "# 2. Load previous alignment and lexicons\n",
    "ali = exkaldi.load_index_table(os.path.join(path_dataset,\"train_lda_mllt\",\"*final.ali\"),useSuffix=\"ark\")\n",
    "lexicons = exkaldi.load_lex(os.path.join(path_dataset,\"dict\",\"lexicons.lex\"))\n",
    "# 3. Estimate the primary fMLLR transform matrix\n",
    "print(\"Estiminate the primary fMLLR transform matrixs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform feature\n"
     ]
    }
   ],
   "source": [
    "fmllrTransMat = exkaldi.hmm.estimate_fMLLR_matrix(\n",
    "                                            aliOrLat=ali,\n",
    "                                            lexicons=lexicons, \n",
    "                                            aliHmm=os.path.join(path_dataset,\"train_lda_mllt\",\"final.mdl\"), \n",
    "                                            feat=ldaFeat,\n",
    "                                            spk2utt=os.path.join(path_dataset,\"data\",\"train\",\"spk2utt\"),\n",
    "                                            outFile=os.path.join(path_dataset,\"train_sat\",\"trans.ark\"),\n",
    "                                        )\n",
    "print(\"Transform feature\")\n",
    "fmllrFeat = exkaldi.use_fmllr(\n",
    "                        ldaFeat,\n",
    "                        fmllrTransMat,\n",
    "                        utt2spk=os.path.join(path_dataset,\"data\",\"train\",\"utt2spk\"),\n",
    "                        outFile=os.path.join(path_dataset,\"train_sat\",\"fmllr_feat.ark\"),\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start build a tree\n",
      "Start to build decision tree.\n",
      "Start Time: 2021/05/23-16:12:15\n",
      "Accumulate tree statistics\n",
      "Cluster phones and compile questions\n",
      "Build tree\n",
      "Done to build the decision tree.\n",
      "Saved Final Tree: /home/marrakchi/TFM/TFM_env/Datasets/ExKaldiASRDatasetFormat/train_sat/tree\n",
      "End Time: 2021/05/23-16:15:32\n",
      "Build tree done.\n"
     ]
    }
   ],
   "source": [
    "# -------------- Build the decision tree ------------------------\n",
    "print(\"Start build a tree\")\n",
    "tree = exkaldi.hmm.DecisionTree(lexicons=lexicons, contextWidth=3, centralPosition=1)\n",
    "tree.train(\n",
    "            feat=fmllrFeat, \n",
    "            hmm=os.path.join(path_dataset,\"train_lda_mllt\",\"final.mdl\"), \n",
    "            ali=ali,\n",
    "            topoFile=os.path.join(path_dataset,\"dict\",\"topo\"), \n",
    "            numLeaves=2500,\n",
    "            tempDir=os.path.join(path_dataset,\"train_sat\"), \n",
    "        )\n",
    "tree.save(os.path.join(path_dataset,\"train_sat\",\"tree\"))\n",
    "print(f\"Build tree done.\")\n",
    "del fmllrFeat\n",
    "\n",
    "# ------------- Start training ----------------------\n",
    "# 1. Initialize a triphone HMM object\n",
    "print(\"Initialize a triphone HMM object\")\n",
    "model = exkaldi.hmm.TriphoneHMM(lexicons=lexicons)\n",
    "model.initialize(\n",
    "            tree=tree, \n",
    "            topoFile=os.path.join(path_dataset,\"dict\",\"topo\"),\n",
    "            treeStatsFile=os.path.join(path_dataset,\"train_sat\",\"treeStats.acc\"), \n",
    "        )    \n",
    "print(f\"Initialized a monophone HMM-GMM model: {model.info}.\")\n",
    "\n",
    "# 2. convert the previous alignment\n",
    "print(f\"Transform the alignment\")\n",
    "newAli = exkaldi.hmm.convert_alignment(\n",
    "                                ali=ali,\n",
    "                                originHmm=os.path.join(path_dataset,\"train_lda_mllt\",\"final.mdl\"), \n",
    "                                targetHmm=model, \n",
    "                                tree=tree,\n",
    "                                outFile=os.path.join(path_dataset,\"train_sat\",\"initial.ali\"),\n",
    "                            )\n",
    "\n",
    "# 2. Split data for parallel training\n",
    "transcription = exkaldi.load_transcription(os.path.join(path_dataset,\"data\",\"train\",\"text\"))\n",
    "transcription = transcription.sort()\n",
    "\n",
    "if parallel > 1:\n",
    "    # split feature\n",
    "    ldaFeat = ldaFeat.sort(by=\"utt\").subset(chunks=parallel)\n",
    "    # split transcription depending on utterance IDs of each feat\n",
    "    tempTrans = []\n",
    "    tempAli = []\n",
    "    tempFmllrMat = []\n",
    "    for f in ldaFeat:\n",
    "        tempTrans.append( transcription.subset(keys=f.utts) )\n",
    "        tempAli.append( newAli.subset(keys=f.utts) )\n",
    "        spks = exkaldi.utt_to_spk(f.utts, utt2spk=os.path.join(path_dataset,\"data\",\"train\",\"utt2spk\"))\n",
    "        tempFmllrMat.append( fmllrTransMat.subset(keys=spks) )\n",
    "    transcription = tempTrans\n",
    "    newAli = tempAli\n",
    "    fmllrTransMat = tempFmllrMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the triphone model\n",
      "Do SAT. Transform to fMLLR feature. <spk2utt> and <utt2spk> files are necessary in this case.\n",
      "Start to train triphone model.\n",
      "Start Time: 2021/05/23-16:51:00\n",
      "Convert transcription to int value format.\n",
      "Compiling training graph.\n",
      "Iter >> 1\n",
      "Use the provided initial alignment\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 199.8287 seconds\n",
      "Iter >> 2\n",
      "Skip aligning\n",
      "Estimate fMLLR matrix\n",
      "Transform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 1605.5952 seconds\n",
      "Iter >> 3\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 199.4587 seconds\n",
      "Iter >> 4\n",
      "Skip aligning\n",
      "Estimate fMLLR matrix\n",
      "Transform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 1618.5550 seconds\n",
      "Iter >> 5\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 198.1842 seconds\n",
      "Iter >> 6\n",
      "Skip aligning\n",
      "Estimate fMLLR matrix\n",
      "Transform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 1608.8642 seconds\n",
      "Iter >> 7\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 205.8335 seconds\n",
      "Iter >> 8\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 206.2665 seconds\n",
      "Iter >> 9\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 203.7294 seconds\n",
      "Iter >> 10\n",
      "Aligning data\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 944.5679 seconds\n",
      "Iter >> 11\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 197.7083 seconds\n",
      "Iter >> 12\n",
      "Skip aligning\n",
      "Estimate fMLLR matrix\n",
      "Transform feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 1708.9421 seconds\n",
      "Iter >> 13\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 214.1402 seconds\n",
      "Iter >> 14\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 221.8544 seconds\n",
      "Iter >> 15\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 215.5009 seconds\n",
      "Iter >> 16\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 224.2876 seconds\n",
      "Iter >> 17\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 215.5558 seconds\n",
      "Iter >> 18\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 217.2896 seconds\n",
      "Iter >> 19\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 216.7010 seconds\n",
      "Iter >> 20\n",
      "Aligning data\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 979.4563 seconds\n",
      "Iter >> 21\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 211.7768 seconds\n",
      "Iter >> 22\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 211.7564 seconds\n",
      "Iter >> 23\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 216.2717 seconds\n",
      "Iter >> 24\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 208.3398 seconds\n",
      "Iter >> 25\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 209.8615 seconds\n",
      "Iter >> 26\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 215.5386 seconds\n",
      "Iter >> 27\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 213.4010 seconds\n",
      "Iter >> 28\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 212.7230 seconds\n",
      "Iter >> 29\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 211.8961 seconds\n",
      "Iter >> 30\n",
      "Aligning data\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 1016.4929 seconds\n",
      "Iter >> 31\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 211.9148 seconds\n",
      "Iter >> 32\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 219.7147 seconds\n",
      "Iter >> 33\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 205.1879 seconds\n",
      "Iter >> 34\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 226.7348 seconds\n",
      "Iter >> 35\n",
      "Skip aligning\n",
      "Skip tansforming feature\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 218.1402 seconds\n",
      "Align last time with final model\n",
      "Done to train the triphone model\n",
      "Saved Final Model: /home/marrakchi/TFM/TFM_env/Datasets/ExKaldiASRDatasetFormat/train_sat/final.mdl\n",
      "Saved Ali:  /home/marrakchi/TFM/TFM_env/Datasets/ExKaldiASRDatasetFormat/train_sat/final.ali\n",
      "Saved Feat Transform Matrix:  /home/marrakchi/TFM/TFM_env/Datasets/ExKaldiASRDatasetFormat/train_sat/trans.ark\n",
      "End Time: 2021/05/23-21:26:21\n",
      "GmmHmmInfo(phones=26, pdfs=2152, transitionIds=4390, transitionStates=2187, dimension=40, gaussians=19594)\n"
     ]
    }
   ],
   "source": [
    "# 3. Train\n",
    "print(\"Train the triphone model\")\n",
    "model.train(\n",
    "            ldaFeat,\n",
    "            transcription, \n",
    "            os.path.join(path_dataset,\"dict\",\"L.fst\"), \n",
    "            tree,\n",
    "            tempDir=os.path.join(path_dataset,\"train_sat\"),\n",
    "            initialAli=newAli,\n",
    "            fmllrTransMat=fmllrTransMat,\n",
    "            spk2utt=os.path.join(path_dataset,\"data\",\"train\",\"spk2utt\"), \n",
    "            utt2spk=os.path.join(path_dataset,\"data\",\"train\",\"utt2spk\"),                \n",
    "            numIters=numIters, \n",
    "            maxIterInc=maxIterInc,\n",
    "            totgauss=15000,\n",
    "            realignIter=realignIter,\n",
    "            fmllrIter=fmllrIter,\n",
    "            boostSilence=1.0,\n",
    "            power=0.2,\n",
    "            fmllrSilWt=0.0,       \n",
    "        )\n",
    "print(model.info)\n",
    "del ldaFeat\n",
    "del fmllrTransMat\n",
    "del newAli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "declare.is_file( os.path.join(path_dataset,\"train_sat\",\"final.mdl\") )\n",
    "declare.is_file( os.path.join(path_dataset,\"train_sat\",\"tree\") )\n",
    "model = exkaldi.load_hmm( os.path.join(path_dataset,\"train_sat\",\"final.mdl\") )\n",
    "tree = exkaldi.load_tree( os.path.join(path_dataset,\"train_sat\",\"tree\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to make WFST graph.\n",
      "Load lexicon bank.\n",
      "Generate CLG fst done.\n",
      "Compose HCLG fst done.\n",
      "Load test feature.\n",
      "Feature type is lda+mllt\n",
      "Transform feature\n",
      "Decode the first time with original feature.\n",
      "Estimate the primary fMLLR transform matrix.\n",
      "Transform feature with primary matrix.\n",
      "Decode the second time with primary fmllr feature.\n",
      "Determinize secondary lattice.\n",
      "Estimate the secondary fMLLR transform matrix.\n",
      "Compose the primary and secondary transform matrix.\n",
      "Transform feature with final matrix.\n",
      "Rescore secondary lattice.\n",
      "Determinize secondary lattice.\n",
      "Generate lattice done.\n",
      "Generate reference text done.\n",
      "Now score:\n",
      "Penalty: 0.0, LMWT: 1, PER: 24.4%\n",
      "Penalty: 0.0, LMWT: 2, PER: 24.14%\n",
      "Penalty: 0.0, LMWT: 3, PER: 23.89%\n",
      "Penalty: 0.0, LMWT: 4, PER: 23.65%\n",
      "Penalty: 0.0, LMWT: 5, PER: 23.46%\n",
      "Penalty: 0.0, LMWT: 6, PER: 23.25%\n",
      "Penalty: 0.0, LMWT: 7, PER: 23.08%\n",
      "Penalty: 0.0, LMWT: 8, PER: 22.92%\n",
      "Penalty: 0.0, LMWT: 9, PER: 22.81%\n",
      "Penalty: 0.0, LMWT: 10, PER: 22.74%\n",
      "Penalty: 0.5, LMWT: 1, PER: 24.39%\n",
      "Penalty: 0.5, LMWT: 2, PER: 24.12%\n",
      "Penalty: 0.5, LMWT: 3, PER: 23.84%\n",
      "Penalty: 0.5, LMWT: 4, PER: 23.64%\n",
      "Penalty: 0.5, LMWT: 5, PER: 23.41%\n",
      "Penalty: 0.5, LMWT: 6, PER: 23.21%\n",
      "Penalty: 0.5, LMWT: 7, PER: 23.01%\n",
      "Penalty: 0.5, LMWT: 8, PER: 22.87%\n",
      "Penalty: 0.5, LMWT: 9, PER: 22.82%\n",
      "Penalty: 0.5, LMWT: 10, PER: 22.77%\n",
      "Penalty: 1.0, LMWT: 1, PER: 24.38%\n",
      "Penalty: 1.0, LMWT: 2, PER: 24.11%\n",
      "Penalty: 1.0, LMWT: 3, PER: 23.83%\n",
      "Penalty: 1.0, LMWT: 4, PER: 23.59%\n",
      "Penalty: 1.0, LMWT: 5, PER: 23.38%\n",
      "Penalty: 1.0, LMWT: 6, PER: 23.16%\n",
      "Penalty: 1.0, LMWT: 7, PER: 23.0%\n",
      "Penalty: 1.0, LMWT: 8, PER: 22.91%\n",
      "Penalty: 1.0, LMWT: 9, PER: 22.84%\n",
      "Penalty: 1.0, LMWT: 10, PER: 22.8%\n",
      "Score done. Save the best result.\n"
     ]
    }
   ],
   "source": [
    "# ------------- Compile WFST training ----------------------\n",
    "# Make a WFST decoding graph\n",
    "make_WFST_graph(\n",
    "            outDir=os.path.join(path_dataset,\"train_sat\",\"graph\"),\n",
    "            hmm=model,\n",
    "            tree=tree,\n",
    "        )\n",
    "# Decode test data\n",
    "GMM_decode_fmllr_and_score(\n",
    "                    outDir=os.path.join(path_dataset,\"train_sat\",f\"decode_{order}grams\"), \n",
    "                    hmm=model,\n",
    "                    HCLGfile=os.path.join(path_dataset,\"train_sat\",\"graph\",f\"HCLG.{order}.fst\"),\n",
    "                    tansformMatFile=os.path.join(path_dataset,\"train_lda_mllt\",\"trans.mat\"),\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a DNN acoustic model with fmllr feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "LDAsplice = 3 # Splice how many frames to head and tail for LDA feature. \n",
    "randomSeed = 1234 # Random seed\n",
    "batchSize = 128 # Mini btach size\n",
    "gu = \"all\" # Use GPU\n",
    "epoch = 30 # Epoches\n",
    "testStartEpoch = 5 #Start to evaluate test dataset.\n",
    "dropout = 0.2\n",
    "useCMVN = False #Wether apply CMVN to fmllr feature.\n",
    "splice = 10 # Splice how many frames to head and tail for Fmllr feature.\n",
    "delta = 2 #Wether add delta to fmllr feature.\n",
    "normalizeFeat = True # Wether to normalize the chunk dataset\n",
    "normalizeAMP = False\n",
    "order = 5 # N-GRAM to use (min 1 | max 6)\n",
    "beam = 13 # Decode beam size\n",
    "latBeam = 6 # Lattice beam size\n",
    "acwt = 0.083333 # Acoustic model weight\n",
    "predictModel = \"\" # If not void, skip training. Do decoding only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_DNN_data():\n",
    "    print(\"Start to prepare data for DNN training\")\n",
    "    assert os.path.isdir(f\"{path_dataset}/train_sat\"), \"Please run previous programs up to SAT training.\"\n",
    "\n",
    "    # Lexicons and Gmm-Hmm model\n",
    "    lexicons = exkaldi.load_lex( f\"{path_dataset}/dict/lexicons.lex\" )\n",
    "    hmm = f\"{path_dataset}/train_sat/final.mdl\"\n",
    "    tree = f\"{path_dataset}/train_sat/tree\"\n",
    "\n",
    "    for Name in [\"train\",\"dev\",\"test\"]:\n",
    "\n",
    "        exkaldi.utils.make_dependent_dirs(f\"{path_dataset}/train_dnn/data/{Name}\", pathIsFile=False)\n",
    "        # Make LDA feature\n",
    "        print(f\"Make LDA feature for '{Name}'\")\n",
    "        feat = exkaldi.load_feat( f\"{path_dataset}/mfcc/{Name}/mfcc_cmvn.ark\" )\n",
    "        feat = feat.splice(left=LDAsplice, right=LDAsplice)\n",
    "        feat = exkaldi.transform_feat(feat, matFile=f\"{path_dataset}/train_lda_mllt/trans.mat\" )\n",
    "        # Compile the aligning graph\n",
    "        print(f\"Compile aligning graph\")\n",
    "        transInt = exkaldi.hmm.transcription_to_int(\n",
    "                                                  transcription=f\"{path_dataset}/data/{Name}/text\",\n",
    "                                                  symbolTable=lexicons(\"words\"),\n",
    "                                                  unkSymbol=lexicons(\"oov\"),\n",
    "                                                )\n",
    "        graphFile = exkaldi.decode.wfst.compile_align_graph(\n",
    "                                            hmm,\n",
    "                                            tree, \n",
    "                                            transcription=transInt,\n",
    "                                            LFile= f\"{path_dataset}/dict/L.fst\",\n",
    "                                            outFile=f\"{path_dataset}/train_dnn/data/{Name}/align_graph\",\n",
    "                                            lexicons=lexicons,\n",
    "                                        )\n",
    "        # Align first time\n",
    "        print(f\"Align the first time\")\n",
    "        ali = exkaldi.decode.wfst.gmm_align(\n",
    "                                        hmm,\n",
    "                                        feat, \n",
    "                                        alignGraphFile=graphFile, \n",
    "                                        lexicons=lexicons,\n",
    "                                    )\n",
    "        # Estimate transform matrix\n",
    "        print(f\"Estimate fMLLR transform matrix\")\n",
    "        fmllrTransMat = exkaldi.hmm.estimate_fMLLR_matrix(\n",
    "                                    aliOrLat=ali,\n",
    "                                    lexicons=lexicons, \n",
    "                                    aliHmm=hmm, \n",
    "                                    feat=feat,\n",
    "                                    spk2utt=f\"{path_dataset}/data/{Name}/spk2utt\",\n",
    "                                )\n",
    "        fmllrTransMat.save( f\"{path_dataset}/train_dnn/data/{Name}/trans.ark\" )\n",
    "        # Transform feature\n",
    "        print(f\"Transform feature\")\n",
    "        feat = exkaldi.use_fmllr(\n",
    "                            feat,\n",
    "                            fmllrTransMat,\n",
    "                            utt2spk=f\"{path_dataset}/data/{Name}/utt2spk\",\n",
    "                        )\n",
    "        # Align second time with new feature\n",
    "        print(f\"Align the second time\")\n",
    "        ali = exkaldi.decode.wfst.gmm_align(\n",
    "                                        hmm,\n",
    "                                        feat,\n",
    "                                        alignGraphFile=graphFile, \n",
    "                                        lexicons=lexicons,\n",
    "                                    )\n",
    "        # Save alignment and feature\n",
    "        print(f\"Save final fmllr feature and alignment\")\n",
    "        feat.save( f\"{path_dataset}/train_dnn/data/{Name}/fmllr.ark\" )\n",
    "        ali.save( f\"{path_dataset}/train_dnn/data/{Name}/ali\" ) \n",
    "        # Transform alignment\n",
    "        print(f\"Generate pdf ID and phone ID alignment\")\n",
    "        ali.to_numpy(aliType=\"pdfID\",hmm=hmm).save( f\"{path_dataset}/train_dnn/data/{Name}/pdfID.npy\" )\n",
    "        ali.to_numpy(aliType=\"phoneID\",hmm=hmm).save( f\"{path_dataset}/train_dnn/data/{Name}/phoneID.npy\" )\n",
    "        del ali\n",
    "        # Compute cmvn for fmllr feature\n",
    "        print(f\"Compute the CMVN for fmllr feature\")\n",
    "        cmvn = exkaldi.compute_cmvn_stats(feat, spk2utt=f\"{path_dataset}/data/{Name}/spk2utt\")\n",
    "        cmvn.save( f\"{path_dataset}/train_dnn/data/{Name}/cmvn_of_fmllr.ark\" )\n",
    "        del cmvn\n",
    "        del feat\n",
    "        # copy spk2utt utt2spk and text file\n",
    "        shutil.copyfile( f\"{path_dataset}/data/{Name}/spk2utt\", f\"{path_dataset}/train_dnn/data/{Name}/spk2utt\")\n",
    "        shutil.copyfile( f\"{path_dataset}/data/{Name}/utt2spk\", f\"{path_dataset}/train_dnn/data/{Name}/utt2spk\")\n",
    "        shutil.copyfile( f\"{path_dataset}/data/{Name}/text\", f\"{path_dataset}/train_dnn/data/{Name}/text\" )\n",
    "        transInt.save( f\"{path_dataset}/data/{Name}/text.int\" )\n",
    "\n",
    "    print(\"Write feature and alignment dim information\")\n",
    "    dims = exkaldi.ListTable()\n",
    "    feat = exkaldi.load_feat( f\"{path_dataset}/train_dnn/data/test/fmllr.ark\" ) \n",
    "    dims[\"fmllr\"] = feat.dim\n",
    "    del feat\n",
    "    hmm = exkaldi.hmm.load_hmm( f\"{path_dataset}/train_sat/final.mdl\" )\n",
    "    dims[\"phones\"] = hmm.info.phones + 1\n",
    "    dims[\"pdfs\"] = hmm.info.pdfs\n",
    "    del hmm\n",
    "    dims.save( f\"{path_dataset}/train_dnn/data/dims\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feat_ali(training=True):\n",
    "\n",
    "    if training:\n",
    "        Name = \"train\"\n",
    "    else:\n",
    "        Name = \"dev\"\n",
    "\n",
    "    feat = exkaldi.load_feat( f\"{path_dataset}/train_dnn/data/{Name}/fmllr.ark\" )\n",
    "\n",
    "    if useCMVN:\n",
    "        cmvn = exkaldi.load_cmvn(f\"{path_dataset}/train_dnn/data/{Name}/cmvn_of_fmllr.ark\")\n",
    "        feat = exkaldi.use_cmvn(feat,cmvn,f\"{path_dataset}/train_dnn/data/{Name}/utt2spk\")\n",
    "        del cmvn\n",
    "    \n",
    "    if delta > 0:\n",
    "        feat = feat.add_delta(delta)\n",
    "\n",
    "    if splice > 0:\n",
    "        feat = feat.splice(splice)\n",
    "\n",
    "    feat = feat.to_numpy()\n",
    "\n",
    "    if normalizeFeat:\n",
    "        feat = feat.normalize(std=True)\n",
    "    \n",
    "    pdfAli = exkaldi.load_ali( f\"{path_dataset}/train_dnn/data/{Name}/pdfID.npy\" )\n",
    "    phoneAli = exkaldi.load_ali( f\"{path_dataset}/train_dnn/data/{Name}/phoneID.npy\" )\n",
    "    \n",
    "    feat.rename(\"feat\")\n",
    "    pdfAli.rename(\"pdfID\")\n",
    "    phoneAli.rename(\"phoneID\")\n",
    "\n",
    "    dataset = exkaldi.tuple_dataset([feat, pdfAli, phoneAli], frameLevel=True)\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator(dataset):\n",
    "\n",
    "    dataIndex = 0\n",
    "    datasetSize = len(dataset)\n",
    "    while True:\n",
    "        if dataIndex >= datasetSize:\n",
    "            random.shuffle(dataset)\n",
    "            dataIndex = 0\n",
    "        one = dataset[dataIndex]\n",
    "        dataIndex += 1\n",
    "        yield one.feat[0], {\"pdfID\":one.pdfID[0], \"phoneID\":one.phoneID[0]}\n",
    "\n",
    "def prepare_test_data(postProbDim):\n",
    "\n",
    "    feat = exkaldi.load_feat( f\"{path_dataset}/train_dnn/data/test/fmllr.ark\" )\n",
    "\n",
    "    if useCMVN:\n",
    "        cmvn = exkaldi.load_cmvn( f\"{path_dataset}/train_dnn/data/test/cmvn_of_fmllr.ark\" )\n",
    "        feat = exkaldi.use_cmvn(feat, cmvn, utt2spk=f\"{path_dataset}/train_dnn/data/test/utt2spk\")\n",
    "        del cmvn\n",
    "\n",
    "    if delta > 0:\n",
    "        feat = feat.add_delta(delta)\n",
    "\n",
    "    if splice > 0:\n",
    "        feat = feat.splice(splice)\n",
    "\n",
    "    feat = feat.to_numpy()\n",
    "    if normalizeFeat:\n",
    "        feat = feat.normalize(std=True)\n",
    "\n",
    "    # Normalize acoustic model output\n",
    "    if normalizeAMP:\n",
    "        ali = exkaldi.load_ali(f\"{path_dataset}/train_dnn/data/train/pdfID.npy\", aliType=\"pdfID\")\n",
    "        normalizeBias = exkaldi.nn.compute_postprob_norm(ali,postProbDim)\n",
    "    else:\n",
    "        normalizeBias = 0\n",
    "    \n",
    "    # ref transcription\n",
    "    trans = exkaldi.load_transcription(f\"{path_dataset}/train_dnn/data/test/text\")\n",
    "    convertTable = exkaldi.load_list_table(path_dataset+'word2phone.map')\n",
    "    trans = trans.convert(convertTable, unkSymbol='<UNK>')   \n",
    "    \n",
    "    return feat, normalizeBias, trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_DNN_model(inputDim,outDimPdf,outDimPho):\n",
    "    \n",
    "    inputs = keras.Input((inputDim,))\n",
    "\n",
    "    ln1 = keras.layers.Dense(1024, activation=None, use_bias=False)(inputs)\n",
    "    ln1_bn = keras.layers.BatchNormalization(momentum=0.95)(ln1)\n",
    "    ln1_ac = keras.layers.ReLU()(ln1_bn)\n",
    "    ln1_do = keras.layers.Dropout(dropout)(ln1_ac)\n",
    "\n",
    "    ln2 = keras.layers.Dense(1024, activation=None, use_bias=False)(ln1_do)\n",
    "    ln2_bn = keras.layers.BatchNormalization(momentum=0.95)(ln2)\n",
    "    ln2_ac = keras.layers.ReLU()(ln2_bn)\n",
    "    ln2_do = keras.layers.Dropout(dropout)(ln2_ac)\n",
    "\n",
    "    ln3 = keras.layers.Dense(1024, activation=None, use_bias=False)(ln2_do)\n",
    "    ln3_bn = keras.layers.BatchNormalization(momentum=0.95)(ln3)\n",
    "    ln3_ac = keras.layers.ReLU()(ln3_bn)\n",
    "    ln3_do = keras.layers.Dropout(dropout)(ln3_ac)\n",
    "\n",
    "    ln4 = keras.layers.Dense(1024, activation=None, use_bias=False)(ln3_do)\n",
    "    ln4_bn = keras.layers.BatchNormalization(momentum=0.95)(ln4)\n",
    "    ln4_ac = keras.layers.ReLU()(ln4_bn)\n",
    "    ln4_do = keras.layers.Dropout(dropout)(ln4_ac)\n",
    "\n",
    "    ln5 = keras.layers.Dense(1024, activation=None, use_bias=False)(ln4_do)\n",
    "    ln5_bn = keras.layers.BatchNormalization(momentum=0.95)(ln5)\n",
    "    ln5_ac = keras.layers.ReLU()(ln5_bn)\n",
    "    ln5_do = keras.layers.Dropout(dropout)(ln5_ac)\n",
    "\n",
    "    outputs_pdf = keras.layers.Dense(outDimPdf,activation=None,use_bias=True,kernel_initializer=\"he_normal\", bias_initializer='zeros', name=\"pdfID\")(ln5_do)\n",
    "\n",
    "    outputs_pho = keras.layers.Dense(outDimPho,activation=None,use_bias=True,kernel_initializer=\"he_normal\", bias_initializer='zeros', name=\"phoneID\")(ln5_do)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=[outputs_pdf,outputs_pho])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_score_test(prob,trans,outDir):\n",
    "\n",
    "    trans.save( os.path.join(outDir,\"ref.txt\") )\n",
    "\n",
    "    hmmFile = f\"{path_dataset}/train_sat/final.mdl\"\n",
    "    HCLGFile = f\"{path_dataset}/train_sat/graph/HCLG.{path_dataset}.fst\"\n",
    "    lexicons = exkaldi.load_lex( f\"{path_dataset}/dict/lexicons.lex\" )\n",
    "    #phoneMap = exkaldi.load_list_table( f\"{args.expDir}/dict/phones.48_to_39.map\" )\n",
    "    phoneMap = exkaldi.load_list_table(path_dataset+'word2phone.map')\n",
    "    #print(\"Decoding...\")\n",
    "    lat = exkaldi.decode.wfst.nn_decode(\n",
    "                                  prob=prob, \n",
    "                                  hmm=hmmFile, \n",
    "                                  HCLGFile=HCLGFile, \n",
    "                                  symbolTable=lexicons(\"words\"),\n",
    "                                  beam=beam,\n",
    "                                  latBeam=latBeam,\n",
    "                                  acwt=acwt,\n",
    "                                  minActive=200,\n",
    "                                  maxActive=7000,\n",
    "                              )\n",
    "    #print(\"Score...\")\n",
    "    minWER = None\n",
    "    for penalty in [0.,0.5,1.0]:\n",
    "        for LMWT in range(1,15,1):\n",
    "\n",
    "            newLat = lat.add_penalty(penalty)\n",
    "            result = newLat.get_1best(lexicons(\"phones\"),hmmFile,lmwt=LMWT,acwt=1,phoneLevel=True)\n",
    "            result = exkaldi.hmm.transcription_from_int(result,lexicons(\"phones\"))\n",
    "            result = result.convert(phoneMap, unkSymbol='<UNK>' )\n",
    "            fileName = f\"{outDir}/penalty_{penalty}_lmwt_{LMWT}.txt\"\n",
    "            result.save( fileName )\n",
    "            score = exkaldi.decode.score.wer(ref=trans, hyp=result, mode=\"present\")\n",
    "            if minWER == None or score.WER < minWER[0]:\n",
    "                minWER = (score.WER, fileName)\n",
    "            #print(f\"{penalty} {LMWT}\",score)\n",
    "\n",
    "    with open( f\"{outDir}/best_PER\", \"w\") as fw:\n",
    "        fw.write( f\"{minWER[0]}% {minWER[1]}\" )\n",
    "\n",
    "    return minWER[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateWER(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, model, feat, bias, trans, outDir):\n",
    "        exkaldi.utils.make_dependent_dirs(outDir, False)\n",
    "        self.model = model\n",
    "        self.feat = feat\n",
    "        self.bias = bias\n",
    "        self.trans = trans\n",
    "        self.outDir = outDir\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        if epoch >= testStartEpoch:\n",
    "            subOutDir = os.path.join( self.outDir, f\"decode_ep{epoch+1}\" )\n",
    "            exkaldi.utils.make_dependent_dirs(subOutDir, False)\n",
    "\n",
    "            prob = {}\n",
    "            for utt, matrix in self.feat.items():\n",
    "                predPdf, predPhone = self.model(matrix, training=False)\n",
    "                prob[utt] = predPdf.numpy() + self.bias\n",
    "                \n",
    "            prob = exkaldi.load_prob(prob)\n",
    "            prob = prob.map(lambda x:exkaldi.nn.log_softmax(x,axis=1))\n",
    "            prob = prob.to_bytes()\n",
    "\n",
    "            WER = decode_score_test(prob, self.trans, subOutDir)\n",
    "\n",
    "            logs['test_WER'] = WER\n",
    "\n",
    "class ModelSaver(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, model, outDir):\n",
    "\n",
    "        self.model = model\n",
    "        self.outDir = outDir\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        outDir = os.path.join(self.outDir,f\"model_ep{epoch+1}.h5\")\n",
    "        self.model.save_weights(outDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_probability():\n",
    "\n",
    "    declare.is_file(predictModel)\n",
    "\n",
    "    dims = exkaldi.load_list_table( f\"{path_dataset}/train_dnn/data/dims\" )\n",
    "    featDim = int(dims[\"fmllr\"])\n",
    "    pdfDim = int(dims[\"pdfs\"])\n",
    "    phoneDim = int(dims[\"phones\"])\n",
    "\n",
    "    # Initialize model\n",
    "    if delta > 0:\n",
    "        featDim *= (delta+1)\n",
    "    if splice > 0:\n",
    "        featDim *= (2*splice+1)\n",
    "\n",
    "    model = make_DNN_model(featDim,pdfDim,phoneDim) \n",
    "    model.load_weights(predictModel)\n",
    "    print(f\"Restorage model from: {predictModel}\")\n",
    "\n",
    "    for Name in [\"train\",\"test\",\"dev\"]:\n",
    "        print(f\"Processing: {Name} dataset\")\n",
    "        feat = exkaldi.load_feat( f\"{path_dataset}/train_dnn/data/{Name}/fmllr.ark\" )\n",
    "\n",
    "        if useCMVN:\n",
    "            print(\"Apply CMVN\")\n",
    "            cmvn = exkaldi.load_cmvn( f\"{path_dataset}/train_dnn/data/{Name}/cmvn_of_fmllr.ark\" )\n",
    "            feat = exkaldi.use_cmvn(feat, cmvn, utt2spk=f\"{path_dataset}/train_dnn/data/{Name}/utt2spk\")\n",
    "            del cmvn\n",
    "\n",
    "        if delta > 0:\n",
    "            print(\"Add delta to feature\")\n",
    "            feat = feat.add_delta(delta)\n",
    "\n",
    "        if splice > 0:\n",
    "            print(\"Splice feature\")\n",
    "            feat = feat.splice(splice)\n",
    "\n",
    "        feat = feat.to_numpy()\n",
    "        if normalizeFeat:\n",
    "            print(\"Normalize\")\n",
    "            feat = feat.normalize(std=True)\n",
    "\n",
    "        outProb = {}\n",
    "        print(\"Forward model...\")\n",
    "        for utt,mat in feat.items():\n",
    "            predPdf, predPhone = model(mat, training=False)\n",
    "            outProb[utt] = exkaldi.nn.log_softmax(predPdf.numpy(),axis=1)\n",
    "\n",
    "        #outProb = exkaldi.load_prob(outProb)\n",
    "        #outProb.save(f\"{args.expDir}/train_dnn/prob/{Name}.npy\")\n",
    "        outProb = exkaldi.load_prob(outProb).to_bytes()\n",
    "        outProb.save(f\"{path_dataset}/train_dnn/prob/{Name}.ark\") \n",
    "        print(\"Save done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Start to prepare data for DNN training\n",
      "Make LDA feature for 'train'\n"
     ]
    }
   ],
   "source": [
    "random.seed(randomSeed)\n",
    "np.random.seed(randomSeed)\n",
    "tf.random.set_seed(randomSeed)\n",
    "# ------------- Prepare data for dnn training ----------------------\n",
    "print('1')\n",
    "if not os.path.isfile(f\"./{path_dataset}/train_dnn/data/dims\"):\n",
    "    prepare_DNN_data()\n",
    "print('2')\n",
    "# ------------- Prepare data for dnn training ----------------------\n",
    "stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "outDir = f\"{path_dataset}/train_dnn/out_{stamp}\"\n",
    "exkaldi.utils.make_dependent_dirs(outDir, pathIsFile=False)\n",
    "print('3')\n",
    "\n",
    "#------------------------ Training and Validation -----------------------------\n",
    "dims = exkaldi.load_list_table( f\"{path_dataset}/train_dnn/data/dims\" )\n",
    "print('4')\n",
    "featDim = int(dims[\"fmllr\"])\n",
    "print('5')\n",
    "pdfDim = int(dims[\"pdfs\"])\n",
    "phoneDim = int(dims[\"phones\"])\n",
    "\n",
    "print('6')\n",
    "  # Initialize model\n",
    "if delta > 0:\n",
    "    featDim *= (delta+1)\n",
    "if splice > 0:\n",
    "    featDim *= (2*splice+1)\n",
    "\n",
    "print('7')\n",
    "if len(predictModel.strip()) == 0:\n",
    "    print('Prepare Data Iterator...')\n",
    "    # Prepare fMLLR feature files\n",
    "    trainDataset = process_feat_ali(training=True)\n",
    "    traindataLen = len(trainDataset)\n",
    "    train_gen = tf.data.Dataset.from_generator(\n",
    "                                        lambda: make_generator(trainDataset),\n",
    "                                        (tf.float32, {\"pdfID\":tf.int32,\"phoneID\":tf.int32})\n",
    "                                ).batch(batchSize).prefetch(3)\n",
    "    steps_per_epoch = traindataLen//batchSize\n",
    "\n",
    "    devDataset = process_feat_ali(training=False)\n",
    "    devdataLen = len(devDataset)\n",
    "    dev_gen = tf.data.Dataset.from_generator(\n",
    "                                        lambda: make_generator(devDataset),\n",
    "                                        (tf.float32, {\"pdfID\":tf.int32,\"phoneID\":tf.int32})\n",
    "                                ).batch(batchSize).prefetch(3)\n",
    "    validation_steps = devdataLen//batchSize\n",
    "\n",
    "    print('Prepare test data')\n",
    "    testFeat, testBias, testTrans = prepare_test_data(postProbDim=pdfDim)\n",
    "    \n",
    "    def train_step():\n",
    "        \n",
    "        model = make_DNN_model(featDim,pdfDim,phoneDim)\n",
    "        model.summary()\n",
    "\n",
    "        model.compile(\n",
    "                    loss = {\n",
    "                            \"pdfID\":keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                            \"phoneID\":keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                        },\n",
    "                    loss_weights = {\"pdfID\":1,\"phoneID\":1},\n",
    "                    metrics = {\n",
    "                                \"pdfID\":keras.metrics.SparseCategoricalAccuracy(),\n",
    "                                \"phoneID\":keras.metrics.SparseCategoricalAccuracy(),\n",
    "                            },\n",
    "                    optimizer = keras.optimizers.SGD(0.08,momentum=0.0),\n",
    "                )\n",
    "\n",
    "        def lrScheduler(epoch):\n",
    "            if epoch > 25:\n",
    "                return 0.001\n",
    "            elif epoch > 22:\n",
    "                return 0.0025\n",
    "            elif epoch > 19:\n",
    "                return 0.005\n",
    "            elif epoch > 17:\n",
    "                return 0.01\n",
    "            elif epoch > 15:\n",
    "                return 0.02\n",
    "            elif epoch > 10:\n",
    "                return 0.04\n",
    "            else:\n",
    "                return 0.08\n",
    "\n",
    "        model.fit(\n",
    "                x = train_gen,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                epochs=epoch,\n",
    "\n",
    "                validation_data=dev_gen,\n",
    "                validation_steps=validation_steps,\n",
    "                verbose=1,\n",
    "\n",
    "                initial_epoch=0,\n",
    "                callbacks=[\n",
    "                            keras.callbacks.EarlyStopping(patience=5, verbose=1),\n",
    "                            keras.callbacks.TensorBoard(log_dir=outDir),\n",
    "                            keras.callbacks.LearningRateScheduler(lrScheduler),\n",
    "                            EvaluateWER(model,testFeat,testBias,testTrans,outDir), \n",
    "                            ModelSaver(model,outDir),         \n",
    "                        ],\n",
    "                    )\n",
    "\n",
    "    print(\"Using GPU: \", gpu)\n",
    "    if gpu != \"all\":\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "        train_step()\n",
    "\n",
    "    else:\n",
    "        my_strategy = tf.distribute.MirroredStrategy()\n",
    "        with my_strategy.scope():\n",
    "            train_step()\n",
    "\n",
    "else:\n",
    "    declare.is_file(predictModel)\n",
    "\n",
    "    model = make_DNN_model(featDim,pdfDim,phoneDim)\n",
    "    model.summary()\n",
    "\n",
    "    model.load_weights(predictModel)\n",
    "\n",
    "    print('Prepare test data')\n",
    "    testFeat, testBias, testTrans = prepare_test_data(postProbDim=pdfDim)\n",
    "    scorer = EvaluateWER(model,testFeat,testBias,testTrans,outDir)\n",
    "\n",
    "    logs = {}\n",
    "    scorer.on_epoch_end(5,logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
